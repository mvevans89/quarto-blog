[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Dithering Data: A Blog",
    "section": "",
    "text": "Series\n\n30 Day Mapping Challenge 2022\nThis series contains the code to make maps from the #30DayMappingChallenge in 2022.\n\n\n\nIndividual Posts\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nMap projections in ggplot\n\n\n\n\n\nDay 19 of the #30DayMapChallenge - Globe\n\n\n\n\n\n\nJan 17, 2023\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nContour maps in ggplot2\n\n\n\n\n\nDay 16 of the #30DayMapChallenge - Minimal\n\n\n\n\n\n\nNov 16, 2022\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nHardee’s and Carl’s Jr. Great Divide\n\n\n\n\n\nDay 15 of the #30DayMapChallenge - Food and Drink\n\n\n\n\n\n\nNov 15, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nMapping space\n\n\n\n\n\nDay 9 of the #30DayMapChallenge - Space\n\n\n\n\n\n\nNov 9, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nCreating a streetmap of Montpellier\n\n\n\n\n\nDay 8 of the #30DayMapChallenge - OpenStreetMap\n\n\n\n\n\n\nNov 8, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nAdding images to plots with ggtext\n\n\n\n\n\nDay 7 of the #30DayMapChallenge - Raster\n\n\n\n\n\n\nNov 7, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nUkraine: The Center of Europe\n\n\n\n\n\nDay 5 of the #30DayMapChallenge - Ukraine\n\n\n\n\n\n\nNov 5, 2022\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nExploring Rainforests with Rayshader\n\n\n\n\n\nDay 4 of the #30DayMapChallenge - Something Green\n\n\n\n\n\n\nNov 4, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nCongressional Districts over time\n\n\n\n\n\nDay 3 of the #30DayMapChallenge - Polygons\n\n\n\n\n\n\nNov 3, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nAM-TRON\n\n\n\n\n\nDay 2 of the #30DayMapChallenge - Lines\n\n\n\n\n\n\nNov 2, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nC’est où, Paris?\n\n\n\n\n\nDay 1 of the #30DayMapChallenge - Points\n\n\n\n\n\n\nNov 1, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nAdding Leading Zeros to Strings\n\n\n\n\n\nHow to add zeros (or other characters) to the front of a string vector\n\n\n\n\n\n\nFeb 27, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nDummy Variables for Dummies\n\n\n\n\n\nHow to create dummy variables within a dataframe\n\n\n\n\n\n\nFeb 27, 2021\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nApplying a function by row\n\n\n\n\n\nSeveral options for applying functions with multiple arguments taken from rows of a dataset\n\n\n\n\n\n\nFeb 23, 2021\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nAccidentally Repeating Labels in ggplot2\n\n\n\n\n\nWhy labels plot over each other and how to avoid it\n\n\n\n\n\n\nJan 27, 2021\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nExtract Data From Spatial Polygons\n\n\n\n\n\nHow to add zeros (or other characters) to the front of a string vector\n\n\n\n\n\n\nFeb 22, 2020\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nRolling Functions Along columns\n\n\n\n\n\nHow to apply aggregate functions that ‘roll’ across data\n\n\n\n\n\n\nFeb 11, 2020\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nThe Packages Wrapped Package\n\n\n\n\n\nA new package to let you know what R packages you use the most\n\n\n\n\n\n\nDec 20, 2019\n\n\n13 min\n\n\n\n\n\n\n  \n\n\n\n\ndplyr and ggplot pipe dreams\n\n\n\n\n\nUsing pipes with dplyr and ggplot2\n\n\n\n\n\n\nJun 15, 2018\n\n\n2 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Michelle Evans, PhD",
    "section": "",
    "text": "Hello! I’m a researcher in disease ecology and public health.\n I work as a Research Associate at Pivot, a non-governmental public health organization based in southeastern Madagascar. We’re working on questions of disease ecology in the context of health system strengthening. I’m interested in how ecologists and public health stakeholders work together to create tools and systems that are useful to folks on the ground. I also like playing around with data, particularly maps.\nFor more information, use the tabs above to navigate to my Rstats blog, academic publications, or check out my CV.\nThis website was created using Quarto."
  },
  {
    "objectID": "posts/2022-02_leading-zeros/index.html",
    "href": "posts/2022-02_leading-zeros/index.html",
    "title": "Adding Leading Zeros to Strings",
    "section": "",
    "text": "We all have that one StackOverflow post that we visit multiple times a week because we just can’t remember exactly how to do that one thing. For me, it is this SO post on adding leading zeros to character strings. This is something I do often when automatically naming files or creating a key/identifier column for a dataset."
  },
  {
    "objectID": "posts/2022-02_leading-zeros/index.html#sorting-data",
    "href": "posts/2022-02_leading-zeros/index.html#sorting-data",
    "title": "Adding Leading Zeros to Strings",
    "section": "Sorting Data",
    "text": "Sorting Data\nLet’s say we want to sort the samples, what will happen?\n\nsort(ids.noAdd)[1:10]\n\n [1] \"sample_1\"   \"sample_10\"  \"sample_100\" \"sample_11\"  \"sample_12\" \n [6] \"sample_13\"  \"sample_14\"  \"sample_15\"  \"sample_16\"  \"sample_17\" \n\n\nsort works by sorting by each digit. This means it groups together all of the samples that begin with 1, even though they represent 1, 10, 100, 1000 etc.\nThis problem goes away when we have added zeros to the beginning of the number, so that they are all four digits:\n\nsort(ids.wZeros)[1:10]\n\n [1] \"sample_0001\" \"sample_0002\" \"sample_0003\" \"sample_0004\" \"sample_0005\"\n [6] \"sample_0006\" \"sample_0007\" \"sample_0008\" \"sample_0009\" \"sample_0010\"\n\n\nsort is still sorting by digit, but it recognizes from the zeros that 1 thru 9 must come before 10."
  },
  {
    "objectID": "posts/2022-02_leading-zeros/index.html#extracting-metadata",
    "href": "posts/2022-02_leading-zeros/index.html#extracting-metadata",
    "title": "Adding Leading Zeros to Strings",
    "section": "Extracting Metadata",
    "text": "Extracting Metadata\nAnother thing we may want to do is subset the character string to easily extract metadata,. Often, we may save samples with names that correspond to a site, block, date, and sample number (e.g. SITEA_BLOCK1_20220227_SAMPLE005). Having all of these identification strings the same length means we can easily extract this information using substr.\nsubstr works by taking the starting and stopping placement of the characters you want to extract and extracting the characters between. So in the example SITEA_BLOCK1_20220227_SAMPLE005, if we wanted to extract the metadata on what block the sample was from, we would extract characters 7 thru 12:\n\nsubstr(\"SITEA_BLOCK1_20220227_SAMPLE005\", start = 7, stop = 12)\n\n[1] \"BLOCK1\"\n\n\nWe can also use substr over a vector of character strings. For example, let’s say we wanted to just extract the number of each sample. This becomes difficult for the vector without added zeros because each sample identification is a different length:\n\ntable(nchar(ids.noAdd))\n\n\n 8  9 10 \n 9 90  1 \n\n\nWe know we need to start the extraction at digit 8, but where we stop depends on the length of each string. One way to deal with this is to supply the number of characters in the string (found using nchar) as the stopping point:\n\nsubstr(ids.noAdd, start = 8, stop = nchar(ids.noAdd))\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"   \"10\"  \"11\"  \"12\" \n [13] \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\"  \"20\"  \"21\"  \"22\"  \"23\"  \"24\" \n [25] \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n [37] \"37\"  \"38\"  \"39\"  \"40\"  \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\" \n [49] \"49\"  \"50\"  \"51\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\" \n [61] \"61\"  \"62\"  \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\"  \"71\"  \"72\" \n [73] \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\"  \"81\"  \"82\"  \"83\"  \"84\" \n [85] \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\"  \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\" \n [97] \"97\"  \"98\"  \"99\"  \"100\"\n\n\nYou can also provide a stopping point that you know is longer than all the strings and substr will just extract as much as it can:\n\nsubstr(ids.noAdd, start = 8, stop = 20)\n\n  [1] \"1\"   \"2\"   \"3\"   \"4\"   \"5\"   \"6\"   \"7\"   \"8\"   \"9\"   \"10\"  \"11\"  \"12\" \n [13] \"13\"  \"14\"  \"15\"  \"16\"  \"17\"  \"18\"  \"19\"  \"20\"  \"21\"  \"22\"  \"23\"  \"24\" \n [25] \"25\"  \"26\"  \"27\"  \"28\"  \"29\"  \"30\"  \"31\"  \"32\"  \"33\"  \"34\"  \"35\"  \"36\" \n [37] \"37\"  \"38\"  \"39\"  \"40\"  \"41\"  \"42\"  \"43\"  \"44\"  \"45\"  \"46\"  \"47\"  \"48\" \n [49] \"49\"  \"50\"  \"51\"  \"52\"  \"53\"  \"54\"  \"55\"  \"56\"  \"57\"  \"58\"  \"59\"  \"60\" \n [61] \"61\"  \"62\"  \"63\"  \"64\"  \"65\"  \"66\"  \"67\"  \"68\"  \"69\"  \"70\"  \"71\"  \"72\" \n [73] \"73\"  \"74\"  \"75\"  \"76\"  \"77\"  \"78\"  \"79\"  \"80\"  \"81\"  \"82\"  \"83\"  \"84\" \n [85] \"85\"  \"86\"  \"87\"  \"88\"  \"89\"  \"90\"  \"91\"  \"92\"  \"93\"  \"94\"  \"95\"  \"96\" \n [97] \"97\"  \"98\"  \"99\"  \"100\"\n\n\nThese two methods work, but only because we want to extract until the end of the string. This would be much more complicated if the stopping point was in a different place for each sample.\nWith the sample ID’s with added zeros, all of the character strings are the same length with 11 digits:\n\ntable(nchar(ids.wZeros))\n\n\n 11 \n100 \n\n\nThis means we can just supply digit 8 as our starting digit and digit 11 as our stopping digit to the substr call, and extract all the sample numbers:\n\nsubstr(ids.wZeros, start = 8, stop = 11)\n\n  [1] \"0001\" \"0002\" \"0003\" \"0004\" \"0005\" \"0006\" \"0007\" \"0008\" \"0009\" \"0010\"\n [11] \"0011\" \"0012\" \"0013\" \"0014\" \"0015\" \"0016\" \"0017\" \"0018\" \"0019\" \"0020\"\n [21] \"0021\" \"0022\" \"0023\" \"0024\" \"0025\" \"0026\" \"0027\" \"0028\" \"0029\" \"0030\"\n [31] \"0031\" \"0032\" \"0033\" \"0034\" \"0035\" \"0036\" \"0037\" \"0038\" \"0039\" \"0040\"\n [41] \"0041\" \"0042\" \"0043\" \"0044\" \"0045\" \"0046\" \"0047\" \"0048\" \"0049\" \"0050\"\n [51] \"0051\" \"0052\" \"0053\" \"0054\" \"0055\" \"0056\" \"0057\" \"0058\" \"0059\" \"0060\"\n [61] \"0061\" \"0062\" \"0063\" \"0064\" \"0065\" \"0066\" \"0067\" \"0068\" \"0069\" \"0070\"\n [71] \"0071\" \"0072\" \"0073\" \"0074\" \"0075\" \"0076\" \"0077\" \"0078\" \"0079\" \"0080\"\n [81] \"0081\" \"0082\" \"0083\" \"0084\" \"0085\" \"0086\" \"0087\" \"0088\" \"0089\" \"0090\"\n [91] \"0091\" \"0092\" \"0093\" \"0094\" \"0095\" \"0096\" \"0097\" \"0098\" \"0099\" \"0100\"\n\n\nWhile there is usually some workaround for when strings are not standardized like this, I find it easier to just standardize from the beginning to avoid any problems later in the workflow. And an easy way to do that is by adding leading 0’s to character strings containing numbers. A good rule of thumb is to always add one more than you think you need (so if you think you will only go up to three digits (eg 999), make it four just in case)."
  },
  {
    "objectID": "posts/2022-02_leading-zeros/index.html#formatc-in-base-r",
    "href": "posts/2022-02_leading-zeros/index.html#formatc-in-base-r",
    "title": "Adding Leading Zeros to Strings",
    "section": "formatC in base R",
    "text": "formatC in base R\nIf you don’t want to add any dependencies, formatC is a function that is in base R that lets you add leading zeros:\n\nformatC(x = 1:100, width = 4, format = \"d\", flag = \"0\")\n\n  [1] \"0001\" \"0002\" \"0003\" \"0004\" \"0005\" \"0006\" \"0007\" \"0008\" \"0009\" \"0010\"\n [11] \"0011\" \"0012\" \"0013\" \"0014\" \"0015\" \"0016\" \"0017\" \"0018\" \"0019\" \"0020\"\n [21] \"0021\" \"0022\" \"0023\" \"0024\" \"0025\" \"0026\" \"0027\" \"0028\" \"0029\" \"0030\"\n [31] \"0031\" \"0032\" \"0033\" \"0034\" \"0035\" \"0036\" \"0037\" \"0038\" \"0039\" \"0040\"\n [41] \"0041\" \"0042\" \"0043\" \"0044\" \"0045\" \"0046\" \"0047\" \"0048\" \"0049\" \"0050\"\n [51] \"0051\" \"0052\" \"0053\" \"0054\" \"0055\" \"0056\" \"0057\" \"0058\" \"0059\" \"0060\"\n [61] \"0061\" \"0062\" \"0063\" \"0064\" \"0065\" \"0066\" \"0067\" \"0068\" \"0069\" \"0070\"\n [71] \"0071\" \"0072\" \"0073\" \"0074\" \"0075\" \"0076\" \"0077\" \"0078\" \"0079\" \"0080\"\n [81] \"0081\" \"0082\" \"0083\" \"0084\" \"0085\" \"0086\" \"0087\" \"0088\" \"0089\" \"0090\"\n [91] \"0091\" \"0092\" \"0093\" \"0094\" \"0095\" \"0096\" \"0097\" \"0098\" \"0099\" \"0100\"\n\n\nThe arguments you provide are:\n\nx the vector of numbers or strings you would like to add leading 0’s to\nwidth the final width you would like each string to have\nformat what class you want the output to be. \"d\" is for integers (default)\nflag signifies what modification you will be doing to x. \"0\" adds leading zeros\n\nPros\n\nvery fast because it is based in C\nunderstandable for those who are already used to C formats\n\nCons\n\nlanguage is not intuitive to those not familiar with C\nthe modification is limited by those provided by flag, so cannot add other characters as leading characters"
  },
  {
    "objectID": "posts/2022-02_leading-zeros/index.html#using-the-str_pad-function-from-the-stringr-package",
    "href": "posts/2022-02_leading-zeros/index.html#using-the-str_pad-function-from-the-stringr-package",
    "title": "Adding Leading Zeros to Strings",
    "section": "Using the str_pad function from the stringr package",
    "text": "Using the str_pad function from the stringr package\nIf you don’t mind using another package, then a great option is the stringr package. It is part of the tidyverse, which means it comes with a lot of supporting documentation for string manipulations.\nThe str_pad function from this pacakge is made for exactly this purpose, “padding” or adding characters to a string:\n\nlibrary(stringr)\nstr_pad(string = 1:100, width = 4, side = \"left\", pad = \"0\")\n\n  [1] \"0001\" \"0002\" \"0003\" \"0004\" \"0005\" \"0006\" \"0007\" \"0008\" \"0009\" \"0010\"\n [11] \"0011\" \"0012\" \"0013\" \"0014\" \"0015\" \"0016\" \"0017\" \"0018\" \"0019\" \"0020\"\n [21] \"0021\" \"0022\" \"0023\" \"0024\" \"0025\" \"0026\" \"0027\" \"0028\" \"0029\" \"0030\"\n [31] \"0031\" \"0032\" \"0033\" \"0034\" \"0035\" \"0036\" \"0037\" \"0038\" \"0039\" \"0040\"\n [41] \"0041\" \"0042\" \"0043\" \"0044\" \"0045\" \"0046\" \"0047\" \"0048\" \"0049\" \"0050\"\n [51] \"0051\" \"0052\" \"0053\" \"0054\" \"0055\" \"0056\" \"0057\" \"0058\" \"0059\" \"0060\"\n [61] \"0061\" \"0062\" \"0063\" \"0064\" \"0065\" \"0066\" \"0067\" \"0068\" \"0069\" \"0070\"\n [71] \"0071\" \"0072\" \"0073\" \"0074\" \"0075\" \"0076\" \"0077\" \"0078\" \"0079\" \"0080\"\n [81] \"0081\" \"0082\" \"0083\" \"0084\" \"0085\" \"0086\" \"0087\" \"0088\" \"0089\" \"0090\"\n [91] \"0091\" \"0092\" \"0093\" \"0094\" \"0095\" \"0096\" \"0097\" \"0098\" \"0099\" \"0100\"\n\n\nThe arguments are:\n\nstring the vector of character strings you would like to pad\nwidth the final width of the character string you would like\nside what side of the string you would like to add the padding (left = before the string, right = after)\npad a single character that you would like to use to pad. It will be repeated when more than one digit is added.\n\nPros\n\nmore intuitive language and arguments\nability to pad both sides of the string\nmore flexibility in what character is used to pad compared to formatC\n\nCons\n\nrequires another package to use\nis affected by the scientific penalty option\n\nThe major downside to this package is that numbers will resort to the scientific notation (e.g. 1e10) if their digits are more than the scientific penalty option defined in R:\n\nex1 <- c(1000000000)\nstr_pad(ex1, width = 13, side = \"left\", pad = \"0\")\n\n[1] \"000000001e+09\"\n\n\nIn comparison, if you use formatC, it will keep all of the zeros:\n\nformatC(ex1, width = 13, format = \"d\", flag = \"0\")\n\n[1] \"0001000000000\"\n\n\nI usually have my scientific penalty essentially turned off (options(scipen=999)) by default because I find it easier to problem-check my own data. But you could also change your options just for this bit of code if you’d like to keep your scientific penalty threshold:\n\nwith(options(scipen = 999),\n     str_pad(ex1, width = 13, side = \"left\", pad = \"0\")\n     )\n\n[1] \"0001000000000\""
  },
  {
    "objectID": "posts/2022-02_leading-zeros/index.html#comparing-performance",
    "href": "posts/2022-02_leading-zeros/index.html#comparing-performance",
    "title": "Adding Leading Zeros to Strings",
    "section": "Comparing Performance",
    "text": "Comparing Performance\nTo me, the trade-off between these two options is the speed of formatC vs. the more intuitive language of str_pad for us non-C users. But just how much faster is formatC?\n\nlibrary(microbenchmark)\n\nmicrobenchmark(\n  pad_c = formatC(x = 1:1e5, width = 8, format = \"d\", flag = \"0\"),\n  stringr = with(options(scipen = 999), str_pad(string = 1:1e5, width = 8, side = \"left\", pad = \"0\")),\n  times = 25\n)\n\nUnit: milliseconds\n    expr      min       lq     mean   median       uq      max neval\n   pad_c 27.80324 28.26081 30.56471 28.97134 30.65747 37.34945    25\n stringr 70.66952 71.34314 72.50030 71.74829 72.26938 85.25796    25\n\n\nLooks like formatC is at least twice as fast as using str_pad, so depending on the size of the dataset or how many times you are replicating padding zeros, it may be worth using formatC, even if you do have to look up again which format corresponds to integers!"
  },
  {
    "objectID": "posts/2018-06_ggplot-pipe/index.html",
    "href": "posts/2018-06_ggplot-pipe/index.html",
    "title": "dplyr and ggplot pipe dreams",
    "section": "",
    "text": "I’ve been getting more and more immersed into using tidyverse packages as time goes on. Every now and then I have trouble getting something to work that I could do very quickly in base R, but that seems to be happening less and less as more SO questions use the tidyverse and documentation gets better and better.\nAnyways, I am a little late to the party but I recently discovered that you can pipe directly from your dplyr calls into ggplot2, manipulating and plotting your data all at once. I hate filling my environment with similar objects. These normally end up being something like dataframe1, dataframe2, dataframe3, etc. because I have no creativity when it comes to names and in the end becoming an unusable mess. Using this piping lets you try-out transformations, scaling, or centering and visualize them without having to add extra columns to your dataframe or create new objects.\nHere’s how it works:\nGo about your normal data wrangling with dplyr, maybe creating a new variable with mutate. Then, simply pipe into your ggplot call, using the . to represent the data you are feeding into it:\n\ndiamonds %>%\n  #calculate the volume\n  mutate(volume = x * y * z) %>%\n  #drop crazy outlier\n  filter(volume < 2000) %>%\n  ggplot(data = ., aes (x = cut, y = volume)) +\n    geom_boxplot(aes(fill = cut))\n\n\n\n\nThe other key thing to note is that once you switch over to ggplot you need to use + (plus sign) to pipe and not the magrittr style pipe (%>%). If not, you’ll get an error like this:\n\ndiamonds %>%\n  #calculate the volume\n  mutate(volume = x * y * z) %>%\n  #drop crazy outlier\n  filter(volume < 2000) %>%\n  ggplot(data = ., aes (x = cut, y = volume))  %>% #this is the wrong pipe!\n    geom_boxplot(aes(fill = cut))\n\n#> Error: `data` must be a data frame, or other object coercible by `fortify()`, not an S3 object with class uneval\n#> Did you accidentally pass `aes()` to the `data` argument?\n\nUnfortunately that error isn’t very informative, but it has to with the incorrect pipe usage within the ggplot portion of the call.\nAnd that’s it. Start using this piping and watch your workspace declutter itself!"
  },
  {
    "objectID": "posts/2021-02_dummy-variables/index.html",
    "href": "posts/2021-02_dummy-variables/index.html",
    "title": "Dummy Variables for Dummies",
    "section": "",
    "text": "As you may have guessed from the cliched title that I couldn’t resist, this post is about dummy variables. Specifically, comparing a couple of different ways to create them in R, and seeing which is fastest.\n\nWhat are dummy variables?\nDummy variables, sometimes called indicator variables, are a way to encode categorical variables as numerical (often binomial), so that we can model them in a regression. Luckily, R will often deal with creating these dummy variables under the hood when we use something like lm to model a regression, but this is not true for all methods.\nAs an example, let’s say we are interested in the effect of three different types of crops (corn, wheat, rice) on soil nitrogen content (% N). The dataset may look something like this:\n\nn.sample <- 100000 #how many rows of data will you have?\ncrop.types <- c(\"corn\", \"rice\", \"wheat\") #what are the levels of your categorical variable?\nnitrogen.data <- data.frame(\n  crop_type = crop.types[sample(1:3, n.sample, replace = T)],\n  perc_N = runif(n.sample, min = 0, max =1 )\n)\nhead(nitrogen.data)\n\n  crop_type      perc_N\n1     wheat 0.644644422\n2     wheat 0.902750588\n3     wheat 0.981700801\n4      corn 0.201877224\n5      corn 0.441983573\n6     wheat 0.005537779\n\n\nIn R, we may model this in a regression framework with the following code:\n\nlm(perc_N ~ 0 + crop_type,\n   data = nitrogen.data)\n\nThis corresponds to a regression equation that may look something like this:\n\\(y_i = \\beta_{1}corn_i + \\beta_2wheat_i + \\beta_3rice_i +\\epsilon\\)\nwhere:\n\n\\(y_i\\) is the % N of the field, i \n\\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) are the coefficients for the effects of corn, wheat, and rice, respectively\n\\(corn_i\\), \\(wheat_i\\), \\(rice_i\\) are all binomial variables (0/1) for the type of crop grown in field i.\n\nThe model formula in the call to lm looks much simpler than this. That is because, under the hood, R is creating dummy variables from our crop_type variable, creating a binomial variable for each level of our categorical variable, without us needing to specify it.\n\nWhen recording our data, it is much easier to do it like the data frame on the left, which is more human readable. Then we can turn it into the data frame on the right programmatically using R. This blog post will go over several ways to do this:\n\nthe model.matrix function in base R\nvia the pivot_wider function of the tidyr package (tidyverse framework)\nthe fastDummiespackage\n\nAnd then compare the speeds on a larger dataset.\n\n\nUsing base R\nIf you don’t want to install any additional packages, you can do this in base R via the model.matrix function. It takes as an argument a model formula, but note that you need to add the ~ 0 + to the formula so that it doesn’t include an intercept when creating the dummy data frame. This will create a matrix without the response variable, so you will may want to join it back to the original dataset.\n\nbase.mat <- model.matrix(object = ~0 + crop_type, data = nitrogen.data)\n#join with original data\nbase.df <- cbind(nitrogen.data, base.mat) \n\nhead(base.df)\n\n  crop_type      perc_N crop_typecorn crop_typerice crop_typewheat\n1     wheat 0.644644422             0             0              1\n2     wheat 0.902750588             0             0              1\n3     wheat 0.981700801             0             0              1\n4      corn 0.201877224             1             0              0\n5      corn 0.441983573             1             0              0\n6     wheat 0.005537779             0             0              1\n\n\nThis is essentially what is happening behind the scenes when you do a linear regression without creating a dummy variable first. In fact the formula used in this call is the same as the one above.\n\n\nUsing the tidyverse\nYou can also do this using the pivot_wider function in tidyr. Using this method, you create a dummy_value column with all 1’s and then spread that value across each of the categories from the names_from column.\n\nlibrary(dplyr)\nlibrary(tidyr)\npivot_df <- nitrogen.data %>%\n  #create dummy value column\n  mutate(dummy_val = 1) %>%\n  #spread across crop type\n  pivot_wider(names_from = crop_type, values_from = dummy_val, values_fill = 0)\n\nhead(pivot_df)\n\n# A tibble: 6 × 4\n   perc_N wheat  corn  rice\n    <dbl> <dbl> <dbl> <dbl>\n1 0.645       1     0     0\n2 0.903       1     0     0\n3 0.982       1     0     0\n4 0.202       0     1     0\n5 0.442       0     1     0\n6 0.00554     1     0     0\n\n\nNote that we also supply the value to fill in a cell when a row doesn’t correspond to that category, via the values_fill argument.\n\n\nUsing the fastDummies package\nThe play dataset we are using for this is only 100,000 rows and we are only concerned with one categorical variable with three levels, but real data may have many more rows and more complicated categorical variables, which can take much longer to turn into dummy variables. Hence the fastDummies package.\nUsing this package is super simple, and relies on one function to create dummy variables across columns. The function will by default turn all character or categorical variables into dummy variables, but you can also specify the columns you would like to “dummify” with the select_columns argument.\n\nlibrary(fastDummies)\nfastdummy.df <- dummy_cols(nitrogen.data, select_columns = \"crop_type\")\n\nhead(fastdummy.df)\n\n  crop_type      perc_N crop_type_corn crop_type_rice crop_type_wheat\n1     wheat 0.644644422              0              0               1\n2     wheat 0.902750588              0              0               1\n3     wheat 0.981700801              0              0               1\n4      corn 0.201877224              1              0               0\n5      corn 0.441983573              1              0               0\n6     wheat 0.005537779              0              0               1\n\n\n\n\nComparing the speed\nThere isn’t much difference between these methods code-wise, except the tidyverse method which is a couple of lines longer. Using pivot_wider also probably scales-up the worst, as you will need to repeat the technique for every categorical variable column you have, while the other methods allow you specify additional columns in the function itself. So we’ll use microbenchmark to see which method is fastest when applied to a dataset of 100k rows and a categorical variable with 26 levels.\n\nn.sample = 100000\nfast.data <- data.frame(response = rnorm(n.sample, 0,1),\n                        category = sample(LETTERS, n.sample, replace = T))\n\ntime.test <- microbenchmark(\n  base = cbind(fast.data, model.matrix(object = ~0 + category, data = fast.data)),\n  tidyr_way = fast.data %>% mutate(dummy_val = 1) %>% pivot_wider(names_from = category, values_from = dummy_val, values_fill = 0),\n  fast_dummies = dummy_cols(fast.data, select_columns = \"category\"),\n  times = 20\n)\n\n\n\nUnit: milliseconds\n         expr       min        lq     mean    median       uq      max neval\n         base 901.39387 914.17483 949.7155 968.14582 980.6932 986.7969    20\n    tidyr_way  47.21551  50.41922  81.0884  62.09486 116.2861 144.5522    20\n fast_dummies 128.78485 139.49679 174.9662 190.12834 199.6670 225.3375    20\n\n\n\n\n\nSurprisingly, the method using tidyr was actually the fastest way to create dummy variables, even though tidyverse is often critiqued for being relatively slow. It also required the most code and wasn’t as clean of a workflow as the others, so there is a trade-off there. It was only about 3x slower than fastDummies, which is another relatively fast option when creating dummy variables for a larger dataset.\nI personally like the ability to pipe into the tidyr method within a tidyverse workflow and will probably continue to use that since it is the fastest, even if it is a couple of more lines of code."
  },
  {
    "objectID": "posts/2021-02_apply-row/index.html",
    "href": "posts/2021-02_apply-row/index.html",
    "title": "Applying a function by row",
    "section": "",
    "text": "I often run into a problem where I have a dataframe of arguments that I would like to provide to a function, running it iteratively over each row. It is a difficult problem to describe, and therefore to search for solutions to on StackOverflow, but it comes up most often when doing something like a parameter search when fitting a model or running simulations. The best way I can describe it as a “row-wise apply statement”.\nLet’s use the iris dataset as an example. In this example, I’m interested in creating a subset of the data corresponding to a certain size bracket of each species. Because the three species in the iris set are really different, the range of sizes I am interested in subsetting differs by species:\nSo for the setosa species, I am interested in plants with a petal width between 0 - 0.15, for the versicolor species, I am interested in plants with a petal width between 1 - 1.20, and for virginicia, I am interested in plants with a petal width between 2 - 2.50. This is a toy example with only three rows, and we could easily accomplish this by combining multiple instances of the subset function.\nThis would become tedious as you have more subsets to do, so you may write a function to reduce how much you are hardcoding.\nThen you could simply run this and change what row you are referring to:\nWritten this way, it is clear that all we are changing for each subset is the index number in the square brackets. This is clue that we can “vectorize” this process to make it easier to read and faster to run. There’s a lot on the internet on vectorization and functionals, and I recommend Noam Ross’s blog post on this as a good starting place."
  },
  {
    "objectID": "posts/2021-02_apply-row/index.html#old-way-a-for-loop",
    "href": "posts/2021-02_apply-row/index.html#old-way-a-for-loop",
    "title": "Applying a function by row",
    "section": "Old way: a for loop",
    "text": "Old way: a for loop\nOld me would have just written a for-loop for this. I like for-loops for their readability and the time spent figuring out how to write an apply statement wasn’t usually worth the time I would be saving. The classic trade-off of investing time to learn how to do something faster in R vs. just writing what you know even if it is slower. A standard for-loop would look something like this:\n\n#create a dataframe that we will fill in\n#use iris so it has the proper column names and classes\nsubset.df <- iris[0,]\nfor(i in 1:nrow(size.brackets)){\n  this.subset <- width_func(size.brackets$species[i], size.brackets$min_width[i],\n                            size.brackets$max_width[i])\n  \n  subset.df <- rbind(subset.df, this.subset)\n}\n\nWe’ve replaced the numbers in the [ ] index with an i, and it changes the numbers from 1 to 2 to 3 programmatically as it works through the for-loop, rather than needing to type it all ourselves. But this can also be done using the family of apply statements, also called ‘functionals’."
  },
  {
    "objectID": "posts/2021-02_apply-row/index.html#lapply",
    "href": "posts/2021-02_apply-row/index.html#lapply",
    "title": "Applying a function by row",
    "section": "lapply",
    "text": "lapply\nOne option is to use the lapply function. Admittedly, this way is pretty hacky, but does get the job done. It is similar to a for-loop, in that what we will provide the lapply function to “apply over” is actually a vector of the row numbers we want to loop over.\n\nlapply.subset <- lapply(1:nrow(size.brackets), \n                        function(r_num) { width_func(species = size.brackets$species[r_num], \n                                                     min_w = size.brackets$min_width[r_num], \n                                                     max_w = size.brackets$max_width[r_num]) })\n\nlapply.subset <- do.call(rbind, lapply.subset)\n\nBecause lapply returns a list, we need that extra line of code to combine all of the subsets into one final dataframe. This way is a nice stepping stone in our thinking from for-loops to apply statements because we are still indexing by each row."
  },
  {
    "objectID": "posts/2021-02_apply-row/index.html#map",
    "href": "posts/2021-02_apply-row/index.html#map",
    "title": "Applying a function by row",
    "section": "Map",
    "text": "Map\nThe less hacky way is to use Map, a function made for this purpose.\n\nmap.subset <- Map(width_func, species = size.brackets$species, \n                  min_w = size.brackets$min_width, max_w = size.brackets$max_width)\n\nmap.subset <- do.call(rbind, map.subset)\n\nAs with lapply, Map returns a list, so we need to use the do.call(rbind,) statement to combine it into a dataframe again. Unlike the other apply statements in R, Map takes the function first, and then we provide the arguments as vectors, by indexing them from the dataframe using the $ operator sign."
  },
  {
    "objectID": "posts/2021-02_apply-row/index.html#which-is-actually-fastest",
    "href": "posts/2021-02_apply-row/index.html#which-is-actually-fastest",
    "title": "Applying a function by row",
    "section": "Which is actually fastest?",
    "text": "Which is actually fastest?\nDoes it actually matter which of these we use? To test that, we will make a larger dataframe and compare the run times.\n\nlibrary(ggplot2); theme_set(theme_bw())\nlibrary(microbenchmark)\n\n#create a large dataset of 3k rows\nsize.large <- size.brackets[rep(1:3, 1000),]\n\ntime.test <- microbenchmark(\n  for_loop = {\n    subset.df <- iris[0,]\n    for(i in 1:nrow(size.large)){\n      this.subset <- width_func(size.large$species[i], size.large$min_width[i],\n                              size.large$max_width[i])\n  \n      subset.df <- rbind(subset.df, this.subset)\n    }},\n  lapply_test = do.call(rbind, lapply(1:nrow(size.large), \n                        function(r_num) { width_func(species = size.large$species[r_num], \n                                                     min_w = size.large$min_width[r_num], \n                                                     max_w = size.large$max_width[r_num]) })),\n  map_test = do.call(rbind, Map(width_func, species = size.large$species, \n                  min_w = size.large$min_width, max_w = size.large$max_width)),\n  times = 10\n)\n\n\n\nUnit: milliseconds\n        expr       min        lq      mean    median        uq       max neval\n    for_loop 7746.9349 7871.4320 8508.4656 8042.4870 9386.8123 9896.5862    10\n lapply_test  663.9130  671.9884  752.5996  737.7693  811.9132  881.6685    10\n    map_test  657.8118  670.5947  733.5510  676.6685  823.5606  874.4669    10\n\n\nCoordinate system already present. Adding new coordinate system, which will replace the existing one.\n\n\n\n\n\nYes, it does look like the for-loop is over 10x slower than the two apply statements. This is mostly caused by the rbind statement we are using to combine the subsets within each loop. At each instance, R is creating a new dataframe with the expanded dimensions, and it is this process that is a drag on memory and time.\nThere isn’t much difference speed-wise between lapply and Map, so which you use is mostly up to personal preference for which you find more intuitive and readable."
  },
  {
    "objectID": "posts/2020-02_extract-polygons/index.html",
    "href": "posts/2020-02_extract-polygons/index.html",
    "title": "Extract Data From Spatial Polygons",
    "section": "",
    "text": "If you work with spatial data, you are probably familiar with the extract function from the raster package, which extracts values from a RasterLayer or RasterStack to polygons or points. This is the type of function you may use when you want to know the elevation at a latitude/longitude value or the mean temperature within a county polygon.\nThis function only works when you value you want to extract is in a raster. If you want to extract from a vector layer, you’ll need to use another operation or function. This technique is a type of vector overlay that is similar to an intersect operation.\n\nI’ll go over the full workflow below, but the function is:\n\nst_join(points, polygons, join = st_intersects)\n\nThis is from the sf package, and you provide it with the point and polygon vector data, both as sf objects.\nAs an example, I’ll go through extracting state names to some random points.\nThe USAboundaries package has spatial data for the US. It can be installed from the ROpenSci repository.\n\n# install.packages(\"USAboundariesData\", repos = \"http://packages.ropensci.org\")\nlibrary(USAboundaries)\nlibrary(USAboundariesData)\nlibrary(sf)\nlibrary(ggplot2)\n\nstates <- us_states()\ncities <- us_cities()\n\n#due to an problem between differnt PROJ version, must manually set the CRS\nst_crs(states) <- 4326\nst_crs(cities) <- 4326\n\nWe’ll just choose 100 cities at random and plot them over the states to get an idea of what this looks like. I’m going to zoom in on the East Coast just for visualization purposes.\n\nsampled.cities <- cities[sample(1:nrow(cities), 100, replace = F),]\n\nggplot() +\n  geom_sf(data = states) +\n  geom_sf(data = sampled.cities) +\n  theme_void() +\n  coord_sf(xlim = c(-95,-60), ylim = c(25,50)) \n\n\n\n\nWe’ll drop some columns so it is easier to see how the extracted values are added to the dataset.\n\nsampled.cities <- dplyr::select(sampled.cities, city, state_name)\n\nhead(sampled.cities)\n\nSimple feature collection with 6 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -123.3595 ymin: 34.20419 xmax: -79.2535 ymax: 44.54197\nCRS:           EPSG:4326\n# A tibble: 6 × 3\n  city          state_name                 geometry\n  <chr>         <chr>                   <POINT [°]>\n1 Wahoo         Nebraska       (-96.61662 41.21499)\n2 Cincinnati    Ohio            (-84.50645 39.1399)\n3 Mullins       South Carolina  (-79.2535 34.20419)\n4 Downers Grove Illinois       (-88.02123 41.79503)\n5 Philomath     Oregon         (-123.3595 44.54197)\n6 Edgerton      Wisconsin      (-89.07108 42.83823)\n\n\nWe can then extract the values held in the polygons to the points.\n\npoint.extraction <- st_join(sampled.cities, states, join = st_intersects)\n\nhead(point.extraction)\n\nSimple feature collection with 6 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -123.3595 ymin: 34.20419 xmax: -79.2535 ymax: 44.54197\nCRS:           EPSG:4326\n# A tibble: 6 × 15\n  city    state…¹             geometry statefp statens  affgeoid    geoid stusps\n  <chr>   <chr>            <POINT [°]> <chr>   <chr>    <chr>       <chr> <chr> \n1 Wahoo   Nebras… (-96.61662 41.21499) 31      01779792 0400000US31 31    NE    \n2 Cincin… Ohio     (-84.50645 39.1399) 39      01085497 0400000US39 39    OH    \n3 Mullins South …  (-79.2535 34.20419) 45      01779799 0400000US45 45    SC    \n4 Downer… Illino… (-88.02123 41.79503) 17      01779784 0400000US17 17    IL    \n5 Philom… Oregon  (-123.3595 44.54197) 41      01155107 0400000US41 41    OR    \n6 Edgert… Wiscon… (-89.07108 42.83823) 55      01779806 0400000US55 55    WI    \n# … with 7 more variables: name <chr>, lsad <chr>, aland <dbl>, awater <dbl>,\n#   state_name.y <chr>, state_abbr <chr>, jurisdiction_type <chr>, and\n#   abbreviated variable name ¹​state_name.x\n\n\nThis extracts all of the columns from the states object to the city points, but you can also specify which columns you want by using select nested within the st_join function.\n\nstate.abbr.extraction <- st_join(sampled.cities, dplyr::select(states, state_abbr), \n                                 join = st_intersects)\n\nhead(state.abbr.extraction)\n\nSimple feature collection with 6 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -123.3595 ymin: 34.20419 xmax: -79.2535 ymax: 44.54197\nCRS:           EPSG:4326\n# A tibble: 6 × 4\n  city          state_name                 geometry state_abbr\n  <chr>         <chr>                   <POINT [°]> <chr>     \n1 Wahoo         Nebraska       (-96.61662 41.21499) NE        \n2 Cincinnati    Ohio            (-84.50645 39.1399) OH        \n3 Mullins       South Carolina  (-79.2535 34.20419) SC        \n4 Downers Grove Illinois       (-88.02123 41.79503) IL        \n5 Philomath     Oregon         (-123.3595 44.54197) OR        \n6 Edgerton      Wisconsin      (-89.07108 42.83823) WI        \n\n\nJust remember that when you want to “extract” polygons with sf what you are really doing is taking the results of a spatial join where the polygons intersect."
  },
  {
    "objectID": "posts/2020-02_rolling-functions/index.html",
    "href": "posts/2020-02_rolling-functions/index.html",
    "title": "Rolling Functions Along columns",
    "section": "",
    "text": "Rolling, or window, functions allow you to apply a function over a window of size n of a vector, such as a column in a dataframe. This is especially useful when you want to know the accumulation of some variable over time, like precipitation over the past month or the count of some occurrence in a surrounding number of days.\nTo follow this you’ll need the following packages:\n\nlibrary(RcppRoll)\nlibrary(ggplot2); theme_set(theme_bw())\nlibrary(dplyr)\n\nI recently used rolling functions to deal with a data puzzle of my own. We had case data for a respiratory disease across multiple states and wanted to identify the start of the oubreak in each state. In this case, a state was considered in an outbreak after three consecutive days of new cases. The data looked something like this:\n\nset.seed(8675309)\n#create simulated case data\ncase.data <- data.frame(expand.grid(state = c(\"Georgia\", \"Florida\", \"North Carolina\", \"Alabama\", \"Mississippi\"),\n  day = seq(1,50))) %>%\n  #simulate cases\n  rowwise() %>%\n  mutate(cases = rpois(1,lambda = day^(1/3)))\n\n#randomly add some zeros to the beginning of the time series\nfor (i in 1:length(unique(case.data$state))){\n  this.zero.index <- sample(1:30, rpois(1,lambda=8))\n  case.data <- mutate(case.data, cases = case_when(\n    state == unique(case.data$state)[i] & day %in% this.zero.index ~ as.integer(0),\n    TRUE ~ cases\n  ))\n}\n\nhead(case.data)\n\n# A tibble: 6 × 3\n# Rowwise: \n  state            day cases\n  <fct>          <int> <int>\n1 Georgia            1     0\n2 Florida            1     0\n3 North Carolina     1     2\n4 Alabama            1     2\n5 Mississippi        1     0\n6 Georgia            2     0\n\n\nAnd this is what the cumulative cases would look like by state:\n\ncase.data %>%\n  group_by(state) %>%\n  mutate(cumul.case = cumsum(cases)) %>%\n  ggplot(aes(x = day, y = cumul.case, color = state)) +\n  geom_line() +\n  xlim(0,15) +\n  ylim(0,35)\n\nWarning: Removed 175 row(s) containing missing values (geom_path).\n\n\n\n\n\nThe steps to finding the first day of an outbreak (i.e. the first day of three consecutive days with cases) are as follows:\n\ncreate a binary column pos_cases where 1 is cases>0 and 0 is cases=0\ncreate a column that is the cumulative sum of pos_cases over the prior three days, ranging from 0 - 3, with 3 signifyingp an outbreak\nfind the first day of the outbreak for each state\n\nUse an ifelse statement to create the new column pos_cases\n\ncase.data <- case.data %>%\n  mutate(pos_cases = ifelse(cases>0,1,0))\n\nhead(case.data)\n\n# A tibble: 6 × 4\n# Rowwise: \n  state            day cases pos_cases\n  <fct>          <int> <int>     <dbl>\n1 Georgia            1     0         0\n2 Florida            1     0         0\n3 North Carolina     1     2         1\n4 Alabama            1     2         1\n5 Mississippi        1     0         0\n6 Georgia            2     0         0\n\n\nUse the roll_sum function from the RcppRoll package to calculate the number of days in the past three days that had cases. I group by state here since we want to do it for each state.\nn corresponds to the window size, here 3 and the alignment of the window is specified as either “center”, “left”, or “right”. The easiest way to translate this to a column is to think of the column as a vector, so “right” would mean the window includes cells before the focal cell.\n\n\n\nA schematic of how different “rolling” functions are applied to data.\n\n\nThe final argument is fill which provides a value to fill in cells that cannot be calculated because their window is incomplete. For this example, it would be days 1 and 2.\n\ncase.data <- case.data %>%\n  group_by(state) %>%\n  arrange(day) %>%\n  mutate(sum_3day = roll_sum(x = pos_cases, n = 3, align = \"right\", fill = NA)) %>%\n  ungroup()\n\narrange(case.data, state, day)[1:8,]\n\n# A tibble: 8 × 5\n  state     day cases pos_cases sum_3day\n  <fct>   <int> <int>     <dbl>    <dbl>\n1 Georgia     1     0         0       NA\n2 Georgia     2     0         0       NA\n3 Georgia     3     3         1        1\n4 Georgia     4     3         1        2\n5 Georgia     5     0         0        2\n6 Georgia     6     2         1        2\n7 Georgia     7     5         1        2\n8 Georgia     8     2         1        3\n\n\nFinally, identify the first day of each outbreak by filtering the dataset only to those days that were the third day of the consecutive three days of cases, finding the minimum day for each state, and subtracting two to get to the start of the outbreak (since it began two days prior to when the outbreak was categorized)\n\noutbreak.day <- case.data %>%\n  filter(sum_3day == 3) %>%\n  group_by(state) %>%\n  summarise(outbreak_day = min(day)-2) %>%\n  ungroup()\n\noutbreak.day\n\n# A tibble: 5 × 2\n  state          outbreak_day\n  <fct>                 <dbl>\n1 Georgia                   6\n2 Florida                   2\n3 North Carolina            3\n4 Alabama                   1\n5 Mississippi               3\n\n\nThe roll family of functions is useful for many data manipulations over time and includes variations such as roll_min, roll_min and roll_max. All work similar to the roll_sum one described above."
  },
  {
    "objectID": "posts/2021-02_geom-labels/index.html",
    "href": "posts/2021-02_geom-labels/index.html",
    "title": "Accidentally Repeating Labels in ggplot2",
    "section": "",
    "text": "When adding labels or text to ggplot you may have run into a problem where it is either taking much longer than you expect to plot, crashing Rstudio while plotting, or creating blurry or jagged text/labels. This all stems from the same problem, which is that, if the labels are repeated n times in your dataset, they are being plotted n times. As it writes those text over and over again on top of itself, it 1) will take a long time and 2) will create blurry text. Below are several options to fix this problem.\nBut first, the problem…\n\nExample of the repeating problem\nLet’s create an example where this problem could arise. For me, it’s when I want to use a summary statistic within a ggplot call. Say we want to look at the frequency of mpg mtcars dataset, for example, and add a label for the median.\n\ndata(mtcars)\n\nggplot(mtcars, aes(x = mpg))+\n  geom_histogram() +\n  geom_vline(aes(xintercept = median(mpg)), color = \"red\", linetype = 'dashed')+\n  geom_text(aes(x = median(mpg), y = 5, label = paste(\" Median =\", median(mpg))), hjust = 0)+\n  ggtitle(\"Distribution of MPG in the mtcars dataset\")\n\n\n\n\nThis dataset is not very long, so it will plot quickly, but you can see that the text for the “Median” label is very jagged. This is because it is plotting that for each row in the dataset, 32 times total. geom_text was originally designed for work with a dataframe where you may want to label each point or observation, and so, following the philosophy of tidy data, repeats this for each row of the initial dataframe.\nHere are three ways to fix the problem, presented in my order of preference:\n\nUse annotate instead of geom_text\nReset the data source before the geom_text call\nCalculate the summary statistic outside of the ggplot function and call it directly\n\nI came across these solutions in a couple of different places, but most were drawn from this stackoverflow post.\n\n\nUse the annotate geom\nPerhaps the most straight-forward solution is to switch to using a geom that is made for this purpose, annotate. From its reference page, “properties of the geoms are not mapped from variables of a data frame, but are instead passed in as vectors”. This solves the repeating problem due to the label being linked to the data source above.\n\nggplot(mtcars, aes(x = mpg))+\n  geom_histogram() +\n  geom_vline(aes(xintercept = median(mpg)), color = \"red\", linetype = 'dashed')+\n  annotate(geom = \"text\", x = median(mtcars$mpg), y = 5, label = paste(\" Median =\", median(mtcars$mpg)), \n           hjust = 0)+\n  ggtitle(\"Distribution of MPG in the mtcars dataset\")\n\n\n\n\nThe important thing to note in the code above is that we must specify the dataset in the annotate call (e.g. mtcars$mpg, instead of just mpg), since it doesn’t use non-standard evaluation (NSE) within the aes like other geoms.\n\n\nReset the data source\nIf you’d like to keep using geom_text, you can reset the data source within its call, so that it is no longer linking labels to the multi-rowed data source used in the original ggplot call. This is done by manually setting the data to an empty data.frame in the geom_text call.\n\nggplot(mtcars, aes(x = mpg))+\n  geom_histogram() +\n  geom_vline(aes(xintercept = median(mpg)), color = \"red\", linetype = 'dashed')+\n  geom_text(data = data.frame(), aes(x = median(mtcars$mpg), y = 5, \n                                     label = paste(\" Median =\", median(mtcars$mpg))), hjust = 0)+\n  ggtitle(\"Distribution of MPG in the mtcars dataset\")\n\n\n\n\nAs in the first solution, you’ll need to specify that mpg is within the mtcars dataframe within the aes, because the dataframe that the geom_text is using is the blank one.\n\n\nCalculate the summary statistic outside\nAnother option is to calculate the median value outside of the ggplot call and then call it directly with geom_text. This is my least favorite because it creates an extra object in your environment, and I am all for a clean environment. Plus the functionality to do this already exists in ggplot, so why not use it?\n\n#create the median value\nmed.mpg <- median(mtcars$mpg)\n\nggplot(mtcars, aes(x = mpg))+\n  geom_histogram() +\n  geom_vline(aes(xintercept = median(mpg)), color = \"red\", linetype = 'dashed')+\n  geom_text(data = data.frame(), aes(x = med.mpg, y = 5, \n                                     label = paste(\" Median =\", med.mpg)), hjust = 0)+\n  ggtitle(\"Distribution of MPG in the mtcars dataset\")\n\n\n\n\nYou could use this method with either geom_text or annotate, but if you are already using annotate you may as well just use the first solution.\n\n\nWhat about geom_label?\nAlthough I only talked about this issue with geom_text, there is another geom that is functionally the same, except it adds a box and background, geom_label. Interestingly this doesn’t have the same blurry problem as geom_text:\n\nggplot(mtcars, aes(x = mpg))+\n  geom_histogram() +\n  geom_vline(aes(xintercept = median(mpg)), color = \"red\", linetype = 'dashed')+\n  geom_label(aes(x = median(mpg), y = 5, label = paste(\" Median =\", median(mpg))), hjust = 0)+\n  ggtitle(\"Distribution of MPG in the mtcars dataset\")\n\n\n\n\nBut is this just because it has a white background and so you can’t see the labels that it is plotting over? One way to check this is to benchmark a larger dataset and compare the times, which we can do with the tictoc package. First, the original plot:\n\nlibrary(tictoc)\n\ntic()\nggplot(mtcars, aes(x = mpg))+\n  geom_histogram() +\n  geom_vline(aes(xintercept = median(mpg)), color = \"red\", linetype = 'dashed')+\n  geom_label(aes(x = median(mpg), y = 5, label = paste(\" Median =\", median(mpg))), hjust = 0)+\n  ggtitle(\"Distribution of MPG in the mtcars dataset\")\n\n\n\ntoc()\n\n0.255 sec elapsed\n\n\n\n#make a larger mtcars dataset using rep\nmtcars.3k <- mtcars[(rep(1:nrow(mtcars), each = 100)),]\n\ntic()\nggplot(mtcars.3k, aes(x = mpg))+\n  geom_histogram() +\n  geom_vline(aes(xintercept = median(mpg)), color = \"red\", linetype = 'dashed')+\n  geom_label(aes(x = median(mpg), y = 500, label = paste(\" Median =\", median(mpg))), hjust = 0)+\n  ggtitle(\"Distribution of MPG in the mtcars dataset\")\n\n\n\ntoc()\n\n10.458 sec elapsed\n\n\nThe larger dataset takes over 25 times longer to plot. Clearly, the problem remains, even if the resulting plot isn’t blurry. The solutions are the same as for geom_text, with the primary difference being that you must specify the type of annotation as ‘label’ instead of ‘text’, when using annotate.\n\ntic()\nggplot(mtcars.3k, aes(x = mpg))+\n  geom_histogram() +\n  geom_vline(aes(xintercept = median(mpg)), color = \"red\", linetype = 'dashed')+\n  annotate(geom = \"label\", x = median(mtcars$mpg), y = 500, label = paste(\" Median =\", median(mtcars$mpg)), \n           hjust = 0)+\n  ggtitle(\"Distribution of MPG in the mtcars dataset\")\n\n\n\ntoc()\n\n0.168 sec elapsed\n\n\nAnd we can double-check that this is doing what we want because the plotting is fast again, less than 0.2 sec!\n\n\nTL;DR\nIf using geom_text or geom_label is creating blurry labels or taking much longer than expected when used with summary statistics, try switching to using annotate."
  },
  {
    "objectID": "posts/2019-12_package-wrapped/index.html",
    "href": "posts/2019-12_package-wrapped/index.html",
    "title": "The Packages Wrapped Package",
    "section": "",
    "text": "As the the decade winds to a close, the internet abounds with various Best of the 2010’s or Best of 2019, from the best TV shows and movies to the best grocery store snacks. You can even get your own personalized ‘Best Of’ list of your most listened to songs and albums on Spotify. But let’s be honest, the majority of my time is not spent watching Award-winning TV, but trying to convince my computer to run that last bit of R code needed for my analysis. Of course, this often involves Spotify and snacks, but that’s another matter. And so, the need for an R Best of 2019 Wrapped was born!\nI whipped up a quick package that parses through all the R and Rmd code on your local machine in specified directories and creates a Spotify-themed image of your most used functions and packages. The powerhouse of the package is a function from the NCmisc package that lists all the functions used in an R script, and identifies what package they are from. I also drew heavily from this very helpful StackOverflow post."
  },
  {
    "objectID": "posts/2019-12_package-wrapped/index.html#purling-your-code",
    "href": "posts/2019-12_package-wrapped/index.html#purling-your-code",
    "title": "The Packages Wrapped Package",
    "section": "purling your code",
    "text": "purling your code\nA limitation of the list.functions.in.file function is that it only works on .R files, not .Rmd. I do most of my work in .Rmd because I like being able to knit to more easily viewable html files, and because the text highlighting makes me feel like Angelina Jolie in Hackers. The purl function from the knitr package extracts all of the R code from these Rmd documents to use later in the workflow. In packageWrapped, this is the Rmd_to_R function.\n\nRmd_to_R(path, temporary.path)\n\nYou provide two locations to this function:\n\npath: the top level directory that you want the function to look for Rmd files in. For me, this is my Dropbox folder where I have all work related things saved.\ntemporary.path: a directory where a folder named package-wrap-purl will be created and all of the R code documents saved in\n\nThe function then creates a list of all the Rmd files contained in the path directory by searching recursively. It then uses the file.mtime function to get the time the file was last modified for each file, and then filters all of the Rmd files to those only modififed in 2019 (for the purpose of the Wrapped 2019).\n\n#find all Rmd files\nfiles <- c(list.files(path, recursive = T, pattern = \"Rmd$\", full.names = T))\n  \n\n#get date info and filter to year of interest\nrmd.files <- data.frame(\n    files = files,\n    #extract date modified\n    date_mod= file.mtime(files)) %>%\n    dplyr::mutate(date_mod = as.Date(date_mod)) %>%\n    dplyr::mutate(year = lubridate::year(date_mod)) %>%\n    #limit to 2019\n    dplyr::filter(year == 2019) %>%\n  #create individual ID codes for each file to save as an R script\n    dplyr::mutate(output = paste0(temporary.path, \"/package-wrap-purl/\", \n                                  date_mod, \"_\", dplyr::row_number(), \".R\")) %>%\n    dplyr::mutate(input = as.character(files))\n\nOnce the dataframe of files to be purled is created with an individual ID for each file to be saved to, the function purl function is mapped to the dataframe using the walk2 function. This is similar to the map2 function from purrr, but is used when what you are interested in is only the “side-effect” of the function, in this case that a new R script is created containing the code in the package-wrapped-purl directory. Because this is doing quite a large operation on many different individuals’ computers, I also wrap the function in possibly. This means the function will be applied to each file, but if it encounters an error, it will simply spit out the supplied error message and move on to the next file. It’s similar to using tryCatch but with a syntax that I find much more intuitive.\n\npurl_func <- purrr::possibly(knitr::purl, otherwise = print(\"error converting Rmd to R. moving to next file\"))\n\npurrr::walk2(rmd.files$input, rmd.files$output, purl_func)"
  },
  {
    "objectID": "posts/2019-12_package-wrapped/index.html#generating-function-and-package-data",
    "href": "posts/2019-12_package-wrapped/index.html#generating-function-and-package-data",
    "title": "The Packages Wrapped Package",
    "section": "Generating function and package data",
    "text": "Generating function and package data\nNow that all the R code is contained in .R files that are easily readable by list.functions.in.file, we simply apply that function over all the files and summarize the resulting lists of functions and packages into something more interesting. This is all done in the check_packs function.\n\npacks.data <- check_packs(path, purled_R_dir, ignore.Rmd = F, load.packages = T, include.default = F)\n\nThe first two arguments should be the same as those provided to the Rmd_to_R function, with purled_R_dir now pointing directly to the package-wrapped-purl directory. The second two arguments are more like checks to make sure you are truly searching all of your files and packages.\n\nignore.Rmd: the default for this is FALSE, in which case the function will also search for any .Rmd files within your PATH, and return how many files there are. This is a reminder to purl those files to R scripts via the Rmd_to_R function. If you’ve already done this, then you can set this argument to TRUE.\nload.packages: is a boolean as to whether all of your installed packages should be loaded or not. The list.functions.in.file function can only indentify a function’s package if that package is loaded, so you ideally want to load all of your packages. The function will do this automatically, if set to TRUE, but you can turn this behavior off if you prefer to choose which packages to load. This is particularly helpful if certain packages take a long time to load or if loading some packages is resulting in errors.\n\nThe final argument, include.default, is whether or not you want to include the seven default packages that come loaded with R. These are base, utils, datasets,grDevices, graphics, stats, and methods. I found that most workflows relied heavily on these packages, especially functions like c, library, and read.csv, and including them made an overall kind of boring list of packages and functions. Feel free to include if you are curious which base functions you use the most.\nThe majority of this function is simply filtering to get out any odd packages or strange text strings returned by the list.functions.in.file function. It again uses list.files to find any .R files, and filters them to those modified in 2019.\n\n  ## get all R files in your directory\n  files <- c(list.files(path, recursive = T, pattern = \"\\\\.R$\", full.names = T),\n             list.files(purled_R_dir, recursive = T, pattern = \"\\\\.R$\", full.names = T))\n\n  #drop any duplicates\n  files <- unique(files)\n\n  #limit to the year (2019)\n  file.times <- file.mtime(files)\n  files <- files[lubridate::year(file.times)==2019]\n\nThe package also drops any files found in packrat directories, since these are supporting files for other packages and presumably not files you have written yourself. This is achieved by searching for character strings in the files using grep.\n\nfiles <- files[!(grepl(\"packrat\", files))]\n\nThe list.functions.in.file function is then applied to each file using the map function, with the original function wrapped in possibly again. Depending on the number of files and length of each, this can take a while. I’ve been finding it’s about 10 files/minutes. Using one of the base apply functions would likely speed this up, but I’ve opted to stick with purrr because of the ease of wrapping the function in possibly and my own reluctance to spend time to finally understand how tryCatch works.\n\nlist.functions.w.error <- purrr::possibly(NCmis::list.functions.in.file, otherwise = NA)\nfuns <- unlist(purrr::map(files, list.functions.w.error))\n\nThe rest of the function is then automating cleaning these function and package names as much as possible. This means dropping any functions that were not associated with a package or were global environment functions (i.e. user created). This is primarily done via functions from the stringr package.\n\n# drop empty ones from the NAs created by possibly\npacks <- packs[packs!=\"\"]\n# \"character\" functions such as reactive objects in Shiny (also functions without packages)\npacks <- packs[!(stringr::str_detect(packs, \"^character\"))]\n# user defined functions in the global environment\npacks <- packs[!(stringr::str_detect(packs, \"^.GlobalEnv\"))]\n\nThe function also goes through and seperates out functions that are found in multiple packages. Unless the notation package::function was used, it is difficult to know which package the function was from, so each package is only counted once from this list. This probably underestimates the counts of certain packages, but including them all very much overestimated graphing packages, such as plotly. Plus it’s just a spotify spoof, so we aren’t going for precision here!\n\n# functions that are in multiple packages' namespaces\nmultipackages<-packs[stringr::str_detect(packs, \", \")]\n\n# get just the  package names from multipackages\nmpackages<-multipackages %>%\n    stringr::str_extract_all(., \"[a-zA-Z0-9]+\") %>%\n    unlist()\n\n# these were just leftover characters from the list\nmpackages <- mpackages[!mpackages %in% c(\"c\", \"package\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\")]\n\n# functions that are from single packages\nupackages <- packs[str_detect(packs, \"package:\") & !packs %in% multipackages] %>%\n    stringr::str_replace(., \"package:\", \"\") %>%\n    stringr::str_replace(., \"[0-9]+$\", \"\") %>%\n    #ggplot gets messed up when you do this so fix it again\n    stringr::str_replace(., \"ggplot\", \"ggplot2\")\n\n#all packages used\nall.packages <- c(mpackages, upackages)\n\nFinally, some summary statistics are created from this information: the frequency of each package, the frequency of each function, and the total number of files included in the search. This makes use of the base function table, which outputs freuency tables.\n\n#get functions and number of times used\nfuns.freq <- data.frame(table(funs)) %>%\n    dplyr::arrange(desc(Freq))"
  },
  {
    "objectID": "posts/2019-12_package-wrapped/index.html#creating-the-image",
    "href": "posts/2019-12_package-wrapped/index.html#creating-the-image",
    "title": "The Packages Wrapped Package",
    "section": "Creating the image",
    "text": "Creating the image\nOnce you have the summary statistics of your package and function use, the next step is image manipulation to make the final Spotify-style image.\n\nplot_wrapped(my_packs, image.path)\n\nThis function takes the results of the last step, created via the check_packs function, and creates a PNG image that will be saved at the provided image.path. I looked into using beamer, which creates slideshow presentations from Rmd files, but ultimately, it seemed easier to get the level of customization I needed by just using base R’s plotting capabilities.\nFirst, is to set up the formatting, specifically the color scheme and font choice. You can use any font in a plot, as long as it is installed locally, by simply defining it as a QuartzFont. You have to provide four fonts, one each for plain, bold, italic, and bolditalic, but since we are just using one font, I provided the same font for each version.\n\nbg.col <- \"#C1F843\" #lime green\ntext.col <- \"#FF35B7\" #pink\n\nquartzFonts(spotify = rep(\"Asimov Regular\", 4))\n\nNext is loading the images that we will be adding manually. These are the R logo and the appropriate hexes for the top packages. The pink and green R logo comes with the packageWrapped package, so it is loaded from there\n\nlogo <- png::readPNG(system.file(\"images/r-logo2.png\", package = \"packageWrapped\"))\n\nHexes are chosen depending on the top packages. A folder of various hex stickers comes with the package, and the appropriate hex is chosen based on a user’s top packages via an internal get_hex function. The current collection is quite small, so if you find that your final image is missing a hex sticker for one of your top packages, please use a PR to add it to the package repo.\nFinally the image is put together by layering different calls to rasterImage and text to add each individual object to the plotting device. This is made somewhat easily modifiable by manually setting the positions for each column and row of objects.\n\n#set alignments\nleft.x = 0.05\nright.x = 0.55\ntop.row.y = 0.50\nbottom.row.y = 0.15\n\npng(image.path, height = 800, width = 800, res = 100, family = 'spotify')\npar(mar = c(0,0,0,0), bg = bg.col, family = \"spotify\")\n#create an empty plot space\nplot(c(0, 1), c(0, 1), ann = F, bty = 'n', type = 'n', xaxt = 'n', yaxt = 'n')\n\n#fun header\nrasterImage(logo,left.x,0.92,left.x+0.06,0.98)\n\n#section titles\ntext(x = left.x+0.06, y = 0.94, \"R Programming Language\",\n       cex = 1.5, col = text.col, adj = c(0,0))\n\ntext(x = 0.70, y = 0.94, \"2019 Wrapped\",\n       cex = 1.5, col = text.col, adj = c(0,0))\n\n\n#top packages\ntext(x =left.x, y = top.row.y, \"TOP PACKAGES\",\n       cex = 1.6, col = text.col, adj = 0)\n\ntext(x =left.x, y = top.row.y-0.05, top.packs.text,\n       cex = 1.6, col = \"black\", adj = c(0,1))\n\n#top functions\ntext(x = right.x , y = top.row.y, \"TOP FUNCTIONS\",\n       cex = 1.6, col = text.col, adj = 0)\n\ntext(x =right.x, y = top.row.y-0.05, top.funs.text,\n       cex = 1.6, col = \"black\", adj = c(0,1))\n\n#files written\ntext(x = left.x, y = bottom.row.y , \"FILES WRITTEN\",\n       cex = 1.6, col = text.col, adj = 0)\n\ntext(x = left.x, y = bottom.row.y - 0.05, paste(my_packs$n.files),\n       cex = 1.6, col = \"black\", adj = c(0,1))\n\n#top genre\ntext(x = right.x , y = bottom.row.y , paste(\"TOP GENRE\"),\n       cex = 1.6, col = text.col, adj = 0)\n\ntext(x = right.x , y = bottom.row.y-0.05 , top.genre,\n       cex = 1.6, col = \"black\", adj = c(0,1))\n\nAdding the individual hex stickers is a little more complicated, because we want them to be nicely aligned no matter how many stickers there are, so a long ifelse statement is used to get the alignment just right. And of course adding a dog meme if there were no matching hex stickers for the user’s top packages.\n\n#add hexes\nif(exists(\"hex.images\")){\n  if (length(hex.images)==3){\n  rasterImage(hex.images[[1]],left.x+0.03,top.row.y+0.05,left.x+0.28,top.row.y+0.35)\n  rasterImage(hex.images[[2]],left.x + 0.33,top.row.y+0.05,left.x+0.58,top.row.y+0.35)\n  rasterImage(hex.images[[3]],left.x + 0.63,top.row.y+0.05,left.x+0.88,top.row.y+0.35)\n  } else if (length(hex.images) ==2){\n    rasterImage(hex.images[[1]],left.x+0.23,top.row.y+0.05,left.x+0.48,top.row.y+0.35)\n    rasterImage(hex.images[[2]],left.x + 0.53,top.row.y+0.05,left.x+0.78,top.row.y+0.35)\n  } else if (length(hex.images==1)){\n    rasterImage(hex.images[[1]],left.x+0.4,top.row.y+0.05,left.x+0.65,top.row.y+0.35)\n  }\n  } else {\n    dog.meme <- jpeg::readJPEG(system.file(\"images/dog_meme.jpg\", package = \"packageWrapped\"))\n    rasterImage(dog.meme, left.x+0.2,top.row.y+0.05,left.x+0.65,top.row.y+0.35)\n  }\n\nThe entire image is then saved to the defined PNG path. This had to be set beforehand because the alignment of everything was very sensitive to the dimensions of the image and allowing it to plot to the default R device and then exporting resulted in some very misaligned text."
  },
  {
    "objectID": "posts/2019-12_package-wrapped/index.html#linking-to-bitbucketgitlabetc.",
    "href": "posts/2019-12_package-wrapped/index.html#linking-to-bitbucketgitlabetc.",
    "title": "The Packages Wrapped Package",
    "section": "Linking to bitbucket/gitlab/etc.",
    "text": "Linking to bitbucket/gitlab/etc.\nA downside to this is that it only searches code that you have locally on your computer. A next step would be to link this to an API on one of these hosting services and search through all your public and private repositories."
  },
  {
    "objectID": "posts/2019-12_package-wrapped/index.html#updating-the-package",
    "href": "posts/2019-12_package-wrapped/index.html#updating-the-package",
    "title": "The Packages Wrapped Package",
    "section": "Updating the package",
    "text": "Updating the package\nThe package can and should be expanded to meet the needs of new users. In particular, missing hex stickers and categorization of packages to calculate the final ‘Top Genre’ are needed. If you find any of your packages are missing, please fill a PR to the repository or file an issue, which you should be able to do anonymously without a bitbucket account."
  },
  {
    "objectID": "posts/2022-30daymapping/day1-points/index.html",
    "href": "posts/2022-30daymapping/day1-points/index.html",
    "title": "C’est où, Paris?",
    "section": "",
    "text": "This is the start of the #30DayMappingChallenge. The first day’s topic is “points”. For this I’m going to mapping all the cities called Paris in the United States. This has a couple of steps:\nHere’s the packages we’ll use:"
  },
  {
    "objectID": "posts/2022-30daymapping/day1-points/index.html#set-up-custom-point-markers",
    "href": "posts/2022-30daymapping/day1-points/index.html#set-up-custom-point-markers",
    "title": "C’est où, Paris?",
    "section": "Set up custom point markers",
    "text": "Set up custom point markers\nThe default point marker in ggplot is a circle, corresponding to shape 20. While you can easily change the shape to any other ready-made shapes in ggplot, I want to use a custom marker of the Eiffel tower. We can try to do this using the geom_image function from ggimage, but we will get an error because ggimage doesn’t work out of the box with sf objects:\n\nggplot() +\n  geom_sf(data = usa.poly) +\n  geom_image(data = paris.coords, image = \"eiffel-tower.png\") +\n  coord_sf(crs = 5070) + \n  theme_void()\n\n#Error in `check_required_aesthetics()`:\n#! geom_image requires the following missing aesthetics: x and y\n\nSo we will need to convert the coordinates into points with x and y coordinates, making sure they are using the same Albers CRS as the US background map:\n\nparis.xy <- paris.coords %>%\n  #reproject\n  st_transform(crs = 5070) %>%\n  #get x and y coordinates\n  mutate(x = st_coordinates(.)[,1],\n         y = st_coordinates(.)[,2]) %>%\n  #drop sf geometry\n  st_drop_geometry()\n\nNow we can try to plot again:\n\nggplot() +\n  geom_sf(data = usa.poly) +\n  geom_image(data = paris.xy, aes(x = x, y = y), image = \"eiffel-tower.png\", size = 0.05) +\n  coord_sf(crs = 5070) + \n  theme_void()"
  },
  {
    "objectID": "posts/2022-30daymapping/day1-points/index.html#turn-the-us-into-a-french-flag",
    "href": "posts/2022-30daymapping/day1-points/index.html#turn-the-us-into-a-french-flag",
    "title": "C’est où, Paris?",
    "section": "Turn the US into a French flag",
    "text": "Turn the US into a French flag\nThe map still feels like it is missing a certain je ne sais quoi, so I will turn the US background into a French flag to give it some flair. To do this, we’ll need to split the US into thirds that we will plot separately. We can split the US into grids using the st_make_grid function to create three evenly spaced grids. The n argument of the function allows us to specify that we want 3 columns and 1 row, and the function will split these into equal size:\n\n#create outline of whole US, dropping shape outlines\n#switch off s2 due to spherical errors\nsf_use_s2(FALSE)\n\nSpherical geometry (s2) switched off\n\nus.outline <- st_union(usa.poly)\n#turn s2 back on\nsf_use_s2(TRUE)\n\nSpherical geometry (s2) switched on\n\nus.grid <- st_make_grid(us.outline, n = c(3,1))\n\nggplot() +\n  geom_sf(data = us.outline) +\n  geom_sf(data = us.grid, fill = NA) +\n  theme_void()\n\n\n\n\nNext we will intersect this grid with the US polygon to create three US shaped polygons:\n\nus.thirds <- as.data.frame(st_intersection(us.outline, us.grid)) %>%\n  #just make color factors\n  mutate(color = c(letters[1:3])) %>%\n  st_as_sf()\n\n#palette of french flag\nfrench_palette <- c(\"#002654\", \"#FFFFFF\", \"#ED2939\")\n\nggplot(us.thirds) +\n  geom_sf(aes(fill = color)) +\n  scale_fill_manual(values = french_palette)\n\n\n\n\nThen we can put it all together, adding some nice small formatting finishing touches. Some of this uses custom fonts via the showtext package, so may not be fully reproducible on your machine.\n\nshowtext_auto()\nggplot() +\n  geom_sf(data = usa.poly, fill = NA, size = 0.1, alpha = 0.4, color = \"gray80\") +\n  geom_sf(data = us.thirds, aes(fill = color), alpha = 0.5, size = 0) +\n  geom_image(data = paris.xy, aes(x = x, y = y), image = \"eiffel-tower.png\", size = 0.04) +\n  geom_sf(data = us.outline, color = \"gray50\", fill = NA) +\n  coord_sf(crs = 5070) + \n  scale_fill_manual(values = french_palette) +\n  guides(fill = 'none') +\n  theme_void() +\n  labs(title = \"C'est où, Paris?\",\n       subtitle = \"Finding the 'City of Light' in the USA\") +\n  theme(plot.title = element_text(family = \"parisish\", size = 16),\n       plot.subtitle = element_text(family = \"parisish\", size = 12))"
  },
  {
    "objectID": "posts/2022-30daymapping/day2-lines/index.html",
    "href": "posts/2022-30daymapping/day2-lines/index.html",
    "title": "Am-tron",
    "section": "",
    "text": "The goal of today is to make a Tron-style map of the Amtrak lines in the US.\nI downloaded the shapefile of Amtrak routes from the US Department of Transportation Open Data Dashboard. But there were some errors with non-overlapping vertices so I ended up just georeferencing it myself in QGIS before starting.\n\nlibrary(ggtext) #html in figure captions\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(showtext) #for tron font\n\n\nlibrary(dplyr)\n\nMost of the work today was just messing about with code to get the aesthetics I wanted. First we load the data:\n\n#georeferenced\namtrak <- st_read(\"amtrak-manual.gpkg\") %>%\n  st_transform(crs = 5070)\n\nReading layer `amtrak-manual' from data source \n  `/Users/mvevans/Dropbox/git/quarto-blog/posts/2022-30daymapping/day2-lines/amtrak-manual.gpkg' \n  using driver `GPKG'\nSimple feature collection with 38 features and 0 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -123.1859 ymin: 25.81329 xmax: -70.00839 ymax: 49.2478\nGeodetic CRS:  WGS 84\n\n#outline of us\nusa.poly <- st_as_sf(maps::map(\"usa\", fill=TRUE, plot =FALSE)) %>%\n  filter(ID == \"main\") %>%\n  st_transform(crs = 5070)\n\nAnd then map it:\n\ncaption.pos <- c(-1.2e6, 2.5e5)\n\namtron <- ggplot() +\n  geom_sf(data = usa.poly, fill = \"gray30\", color = \"gray80\", size = 0, alpha = 0.8) +\n  geom_sf(data = amtrak, color = \"#6EECED\", size = 2.5, alpha = 0.15) +\n  geom_sf(data = amtrak, color = \"#6EECED\", size = 3, alpha = 0.1) +\n  geom_sf(data = amtrak, color = \"#6EECED\", size = 2, alpha = 0.2) +\n  geom_sf(data = amtrak, color = \"#6EECED\", size = 1.5, alpha = 0.2) +\n  geom_sf(data = amtrak, color = \"#6EECED\", size = 1, alpha = 0.5) +\n  geom_sf(data = amtrak, color = \"#6EECED\", size = 0.75, alpha = 0.2) +\n  annotate(\"text\", x = caption.pos[1], y = caption.pos[2], label = \"AMTRON\", color = \"#d3b3f2\", size = 13, \n           family = \"tron\") +\n  annotate(\"text\", x = caption.pos[1], y = caption.pos[2], label = \"AMTRON\", color = \"#6EECED\", size = 14, \n           family = \"tron\", alpha = 0.3) +\n    annotate(\"text\", x = caption.pos[1], y = caption.pos[2], label = \"AMTRON\", color = \"white\", size = 16, \n           family = \"tron\", alpha = 0.1) +\n  coord_sf(clip = \"off\") +\n  labs(caption =  \"**Source:** US DOT, maps package <br>\n       **Created by:**M. Evans\") +\n  theme_void() +\n  theme(plot.background = element_rect(fill = \"black\"),\n        panel.background = element_rect(fill = \"black\"),\n        plot.caption = element_markdown(color = \"gray90\", size = 10, hjust = ),\n        plot.caption.position = \"plot\")\namtron"
  },
  {
    "objectID": "posts/2022-30daymapping/day3-polygons/index.html",
    "href": "posts/2022-30daymapping/day3-polygons/index.html",
    "title": "Congressional Districts over time",
    "section": "",
    "text": "Just in time for Election Day next week, I thought I would map some congressional districts for Day 3 of the #30DayMapChallenge which is focused on polygons.\nThe data for this comes from a database of historical congressional districts hosted by UCLA and created by Jeffrey B. Lewis, Brandon DeVine, Lincoln Pitcher, and Kenneth C. Martis. I downloaded the data for the 70th and 114th Congress and subset out North Carolina because it is a particularly grievous offender of gerrymandering. It was also the focus of a Supreme Court case (Rucho v. Common Cause) that ultimately decided partisan gerrymandering was not something that fell under the realm of the judicial system, and left the map in place. Another aspect of this was the opinion, as Chief Justice John Roberts stated, gerrymandering “is nothing new”. But how do the districts now compare to districts from the 70th Congress, which served from 1927 - 1929.\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(ggtext)\nlibrary(magick)\n\n\nlibrary(dplyr)\n\n\nnc.70 <- st_read(\"ncdist-070.gpkg\")\nnc.114 <- st_read(\"ncdist-114.gpkg\")\n\n#combine into one dataset\nnc.all <- bind_rows(mutate(nc.70, congress = \"70th Congress\"),\n                   mutate(nc.114, congress = \"114th Congress\"))\n#simplify to speed up plotting\nnc.all <- st_simplify(nc.all, preserveTopology = TRUE, dTolerance = 500) %>%\n  #drop some columns for simplicity\n  select(DISTRICT, congress, geom)\n\nMy plan is to animate moving between the two maps to highlight the unnatural gerrymandering of current district polygons. I had originally wanted to do this using the gganimate package but kept running into some issues where it kept hanging up when I tried to animate, so going back to the original magick package that gganimate relies on for now.\nFor this we’ll need to make two separate plots.\n\np.1 <- filter(nc.all, congress == \"70th Congress\") %>%\n  ggplot() +\n  geom_sf(fill = NA, color = \"gray40\") +\n  theme_void() +\n  labs(title = \"Is gerrymandering 'nothing new'?\",\n       subtitle = \"Comparing NC Congressional Districts from 1929 to 2017\",\n       caption = \"#30DayMapChallenge<br>\n        **Source**: Lewis et al. (UCLA 2013)<br>\n       **Created by:**M. Evans\") +\n  annotate(geom = \"text\", x = -82, y = 34.3, label = \"70th Congress\\n(1927-1929)\", family = \"Baskerville\",\n           size = 5)+\n  theme(plot.title = element_text(family = \"Baskerville\", size = 25),\n        plot.subtitle = element_text(family = \"Baskerville\", size = 14),\n        plot.caption = element_markdown(color = \"gray40\", size = 10))\n\np.1\n\n\n\n\n\np.2 <- filter(nc.all, congress == \"114th Congress\") %>%\n  ggplot() +\n  geom_sf(fill = NA, color = \"gray40\") +\n  theme_void() +\n  labs(title = \"Is gerrymandering 'nothing new'?\",\n       subtitle = \"Comparing NC Congressional Districts from 1929 to 2017\",\n       caption = \"#30DayMapChallenge<br>\n        **Source**: Lewis et al. (UCLA 2013)<br>\n       **Created by:**M. Evans\") +\n  annotate(geom = \"text\", x = -82, y = 34.3, label = \"114th Congress\\n(2015-2017)\", family = \"Baskerville\",\n           size = 5)+\n  theme(plot.title = element_text(family = \"Baskerville\", size = 25),\n        plot.subtitle = element_text(family = \"Baskerville\", size = 14),\n        plot.caption = element_markdown(color = \"gray40\", size = 10))\n\np.2\n\n\n\n\nNow animate between the two:\n\nimg <- image_graph(860, 520, res = 100)\nprint(p.1)\n#double image so it stays there a while with the morph\nprint(p.2)\nprint(p.2)\nprint(p.1)\nprint(p.1)\ndev.off()\nanimation <- image_morph(img, frames = 20) %>% image_animate(fps = 10)\n\n\nimage_write_gif(animation, \"nc-anim.gif\")"
  },
  {
    "objectID": "posts/2022-30daymapping/day4-green/index.html",
    "href": "posts/2022-30daymapping/day4-green/index.html",
    "title": "Exploring Rainforests with Rayshader",
    "section": "",
    "text": "library(ggplot2)\nlibrary(sf)\nlibrary(raster)\nlibrary(rayshader)\n\n\nlibrary(dplyr)\n\nToday’s challenge was to map something green. I’ve been meaning to play around with rayshader a bit and had a nice elevation map of Ranomafana, Madagascar on hand so decided that would be a good start. I used a buffer created around a point I chose to get a nice gradient of elevation near Ranomafana.\n\ndem <- raster(\"rano_dem.tif\")\n\n#crop it to zoom in a bit on Ranomafana\nbuffer <- data.frame(lon = 47.2,\n                     lat = -21.255) %>%\n  st_as_sf(coords = c(\"lon\", \"lat\")) %>%\n  st_buffer(dist = 0.1)\n\ndem.crop <- crop(dem, buffer)\nplot(dem.crop)\nplot(buffer, add = T)\n#transform into matrix for rayshader\ndem.mat <- raster_to_matrix(dem.crop)\n\nrayshader comes with some built-in texture/colors for the maps, but I wanted one that was a bit more green given the days’ theme. I created a custom one using create_texture.\n\nforest.texture <- create_texture(lightcolor = \"#163611\",\n                                   shadowcolor = \"#071104\",\n                                   leftcolor = \"#081c05\",\n                                   rightcolor = \"#162d0d\",\n                                   centercolor = \"#228062\")\ndem.mat %>%\n  sphere_shade(sunangle = 35, texture = forest.texture) %>%\n  plot_3d(heightmap = dem.mat, zscale = 30, fov = 0, theta = 45, zoom = 0.7, phi = 30, \n          windowsize = c(1000, 800), water = F, baseshape = \"hex\", background = \"black\")\n\nrender_snapshot(clear = F)\n\n\n\nrender_snapshot(\"rayshade-static.png\", clear = T)\n\nMake it spin! There is a function that does this already, render_movie(), but I wanted mine saved as a gif instead of an avi file, so I used some custom code from Will Bishop.\n\n#source custom function\nlibrary(devtools)\nsource_url(\"https://raw.githubusercontent.com/wcmbishop/rayshader-demo/master/R/rayshader-gif.R\")\nn.frames <- 90\n#theta is angle of spin\nthetas <- seq(0, 360, length.out = n.frames)\n\ndem.mat %>%\n  sphere_shade(sunangle = 35, texture = forest.texture) %>%\n  # plot_3d(heightmap = dem.mat, zscale = 30, fov = 0, zoom = 0.7, phi = 30, \n  #         windowsize = c(1000, 800), water = F, baseshape = \"hex\", background = \"black\") %>%\n\n  save_3d_gif(dem.mat, file = \"spin-ray.gif\", duration = 6,\n              solid = TRUE, shadow = TRUE, zscale = 30,\n              theta = thetas, phi = 30, background = \"black\", baseshape = \"hex\")"
  },
  {
    "objectID": "posts/2022-30daymapping/day5-ukraine/index.html",
    "href": "posts/2022-30daymapping/day5-ukraine/index.html",
    "title": "Ukraine: The Center of Europe",
    "section": "",
    "text": "library(ggplot2)\nlibrary(rnaturalearth)\nlibrary(sf)\nlibrary(ggtext)\nlibrary(showtext)\n\n#add font awesome\nfont_add('fa-brands', here::here('fonts/fa-brands-400.ttf'))\nshowtext_auto()\n\nlibrary(dplyr)\n\nToday’s challenge is a map about Ukraine. I learned recently that there is a point in Ukraine considered the geographic midpoint of Europe. Then, I did some research today and learned this is a highly debated topic, not least of which because the definition of ‘Europe’ really depends on who you talk to. But I was already committed to the idea. For this, I manually georeferenced some points that seemed to be on the edge of Europe, at least to this non-European currently residing in Europe. I then plotted it over a basemap from the rnaturalearth package and used the sf package to add some lines.\n\n#download basemap of the world\nworld.map <- ne_countries(scale = 110, returnclass = 'sf') %>%\n  st_transform(crs = 3035)\n\n#identify center\ncenter <- data.frame(lon = 23.833, lat = 48.5) %>%\n  st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %>%\n  #transform to do in meters\n  st_transform(3035)\n\n#identify edges\nn <- data.frame(lat = 71.095089, lon = 25.783898) %>%\n    st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %>%\n  #transform to do in meters\n  st_transform(3035) %>%\n  mutate(position = \"North\")\nsw <- data.frame(lat = 37.033281, lon = -8.918422) %>%\n    st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %>%\n  #transform to do in meters\n  st_transform(3035) %>%\n  mutate(position = \"Southwest\")\nse <- data.frame(lat = 34.928006, lon = 24.857155) %>%\n    st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %>%\n  #transform to do in meters\n  st_transform(3035) %>%\n  mutate(position = \"Southeast\")\nnw <- data.frame(lat = 65.852065, lon = -23.589617) %>%\n    st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %>%\n  #transform to do in meters\n  st_transform(3035) %>%\n  mutate(position = \"Northwest\")\ne <- data.frame(lat = 57.880922, lon = 56.307665) %>%\n    st_as_sf(coords = c(\"lon\", \"lat\"), crs = 4326) %>%\n  #transform to do in meters\n  st_transform(3035) %>%\n  mutate(position = \"East\")\n\nedges <- bind_rows(n, sw, se, nw, e)\n\nmap.bbox <- st_bbox(st_buffer(edges, 5e5))\n\n#create distance lines\ndist.lines <- st_sfc(mapply(function(a,b){st_cast(st_union(a,b),\"LINESTRING\")}, center$geometry, edges$geometry, SIMPLIFY=FALSE)) %>%\n  st_as_sf(crs = 3035) %>%\n  mutate(length_km = round(st_length(.)/1e3))\n\n#define caption for easier reading\ncaption.lab <- paste0(\"#30DayMapChallenge<br>\",\n                      \"<b>Source: </b><span style='font-family:mono;'>rnaturalearth</span><br>\",\n                      \"<span style='font-family:fa-brands;'>&#xf113;</span> mvevans89\")\n\n\nggplot() +\n  geom_sf(data = world.map, fill = \"gray20\", color = NA, size = 0.6) +\n  geom_sf(data = center, color = \"white\", size = 3) +\n  geom_sf(data = dist.lines, color = \"gray80\", linetype = 21) +\n  geom_sf(data = edges, color = \"white\", size = 2, shape = 21) +\n  coord_sf(xlim = map.bbox[c(1,3)], ylim = map.bbox[c(2,4)]) +\n  annotate(x = st_coordinates(center)[1]+2e5, y = st_coordinates(center)[2]-3e5,\n                                      geom = \"text\", label = \"Rakhiv, Ukraine\\nEurope's Center\\n(Maybe)\",\n           color = \"gray90\", hjust = 0, size = 3.5, family = \"mono\") +\n  geom_richtext(data = data.frame(x = map.bbox[c(1)] - 3e5, \n                           y = map.bbox[c(2)] + 5e4), \n                aes(x = x, y = y, label = caption.lab), fill = NA, label.color = NA,\n                color = \"gray90\", size = 2.5, hjust = 0) +\n  theme(panel.background = element_rect(fill = \"gray40\"),\n        panel.grid.major = element_line(color = \"gray30\"),\n        # axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank())"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Hi, I’m Michelle!",
    "section": "Education",
    "text": "Education\nUniversity of Georgia | Athens, GA  PhD in Integrative Conservation and Ecology | 2015 - 2020  Graduate Certificate in GIS\nWashington University in St. Louis | St. Louis, MO  B.A in African Studies and Ecology | 2007 - 2011"
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Academic Publications",
    "section": "",
    "text": "PDFs that are not open access (indicated by the  icon) are available to download for educational use by clicking on the PDF hyperlink following the title. All others can be downloaded from the journal website via the link provided."
  },
  {
    "objectID": "posts/2022-30daymapping/day7-raster/index.html",
    "href": "posts/2022-30daymapping/day7-raster/index.html",
    "title": "New Zealand Sheep",
    "section": "",
    "text": "Image Credit: Taylor Brandon on Unsplash\n\n\nToday’s goal was to learn about placing some inset images into plots. I had originally planned on using something via ggsave, but because I was just using plain png images, it was easier to insert these using richtext option from the ggtext. Although it is meant to be used to render HTML or markdown text, we can also make use of it’s ability to render HTML to add images.\nFor the raster, I decided to use density of sheep from the Gridded Livestock of the World dataset. This contains global rasters of the density of common livestock and is great for when you want to know just how many sheep are there in New Zealand?\n\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(ggtext)\nlibrary(showtext)\nlibrary(terra)\n\n#add custom fonts\nfont_add('fa-brands', here::here('fonts/fa-brands-400.ttf'))\nfont_add('andika', here::here('fonts/Andika-Regular.ttf'))\nshowtext_auto()\n\nlibrary(dplyr)\n\n\nCreate basemap data\nWhile we don’t end up using a polygon of the outline of New Zealand in the final map, we will use it for cropping the raster to the outline of the country.\n\nnz.poly <- spData::world %>%\n  filter(iso_a2 == \"NZ\") %>%\n  dplyr::select(iso_a2)\n\nWe then load the raster of sheep. I’ve already roughly cropped this to the area surrounding New Zealand to save on file size. We then crop the raster using our outline of New Zealand, making sure we set mask = TRUE so it crops to the outline. If this was set to false, it would only crop it to the extent of the polygon and we would have some messy raster cells nearby. I also transform the scale the number of sheep to be by the 1000. It turns out, New Zealand has a lot of sheep.\n\nsheep <- rast(\"nzSheep.tif\")\n#crop to new zeland outline\nsheep <- crop(sheep, nz.poly, mask = TRUE, touches = TRUE)\nsheep.df <- as.data.frame(sheep, xy = TRUE) %>%\n  mutate(sheep1k = nzSheep/1000)\n\n\n\nCreate the map\nOne frustrating aspect of using a scripted language to create a map is that the fine-tuning of placement and alignment of items can become tedious as you slowly change the numbers corresponding to coordinates to identify the best location. However, a great thing about this is that, once you have the general location sorted, you can automate some of this.\nBelow, I set a kind of architecture for where I’d like my insets to go by defining the base coordinates of the first inset and the spacing between insets, and then calling those objects in the plot. Then, when something changes, I can change the measures in one place instead of at multiple places within the ggplot call. It also cleans up the code in the ggplot call because we just use that one dataframe, insead of an annotate line for each image.\n\n#set colors\nblue.bg <- \"#A4C3D2\"\n#starting sheep coordinates\nsheep.x <- 169.5\nsheep.y <- -37.4\nsheep.width <- 0.75\nsheep.height <- 1.2\n#create dataframe of sheep icon coordinates\nsheep.icons <- data.frame(x = rep(c(sheep.x + (c(1:3)*sheep.width)), 2),\n                          y = rep(c(sheep.y, sheep.y-sheep.height), each = 3),\n                          label = \"<img src='sheep-white.png' width='30'/>\")\n\n#caption label\n#define caption for easier reading\ncaption.lab <- paste0(\"#30DayMapChallenge<br>\",\n                      \"<b>Source: </b>GLW3, Konkapp, DinosoftLabs<br>\",\n                      \"<span style='font-family:fa-brands;'>&#xf113;</span> mvevans89\")\n\n#create sheep plot\nsheep.map <- ggplot() +\n  geom_raster(data = sheep.df, aes(x = x, y = y, fill = sheep1k)) +\n  scale_fill_gradient(high = \"#fff2cb\", low = \"#e7acf2\", \n                      name = bquote(atop(Thousand~Sheep~phantom(),\n                                          per~\"10km\"^2))) +\n  guides(fill = guide_colourbar(title.position=\"top\", title.hjust = 0)) +\n  #add title\n  annotate(geom = \"richtext\", x = 169.5, y = -36, label = \"New Zealand has the<br>highest person:sheep ratio in the world\", family = \"andika\") +\n  #add in little emojis\n  #human\n  annotate(geom = \"richtext\",  label =\"<img src='user-white.png' width='50'/>\",\n           x = 168, y = -38, fill = NA, color = NA) +\n  #colon\n  annotate(geom = \"richtext\", fill = NA, color = NA, text.color = \"white\", label = \":\", \n           family = \"andika\", x = 168.75, y = -38, size = 14) +\n  #add six sheep using dimensions assigned above\n  geom_richtext(data = sheep.icons, aes(x = x, y =y, label = label), fill = NA, color = NA) +\n  # add caption\n  annotate(geom = \"richtext\", x = 178.40, y = -34.5, fill = NA, label.color = NA,\n                color = \"gray20\", size = 2.5, hjust = 1,\n           label = caption.lab) +\n  theme(panel.background = element_rect(\"#A4C3D2\"),\n        panel.grid = element_blank(),\n        axis.text = element_blank(),\n        axis.title = element_blank(),\n        axis.ticks = element_blank(),\n        legend.position = c(0.85,0.1),\n        legend.direction = \"horizontal\",\n        legend.text = element_text(size = 8, color = \"gray20\", family = \"sans\"),\n        legend.title = element_text(color = \"gray20\", family = \"sans\"),\n        legend.background = element_rect(fill = \"#A4C3D2\"))"
  },
  {
    "objectID": "posts/2022-30daymapping/day8-osm/index.html",
    "href": "posts/2022-30daymapping/day8-osm/index.html",
    "title": "Creating a streetmap of Montpellier",
    "section": "",
    "text": "library(ggplot2)\nlibrary(ggtext)\nlibrary(showtext)\nlibrary(sf)\n\nlibrary(osmdata)\n\n#add font awesome\nfont_add('fa-brands', here::here('fonts/fa-brands-400.ttf'))\nfont_add('morris-roman', here::here('fonts/MorrisRoman-Black.ttf'))\nshowtext_auto()\n\nlibrary(dplyr)\n\n\n\n\nImage Source: Marie de Montpellier\n\n\nFor today’s challenge, I wanted to map the historical center of Montpellier, Écusson, using available OpenStreetMap data. Montpellier has one of the largest pedestrian zones in Europe, and it is located in the historical center of the city.\nAlthough there is a way to download data in a scripted way via the osmdata package, I was finding it to be a bit slow. In my impatience, I just downloaded the data manually via overpass turbo. I downloaded the pedestrian ways and the buildings.\n\n#manually downloaded from overpass API\nmtp.path <- st_read(\"mtp-pedestrian.gpkg\") %>%\n  select(surface, old_name, historic)\nsf_use_s2(FALSE)\nmtp.building <- st_read(\"mtp-buildings.gpkg\") %>%\n  select(roof_material, source_heritage,building_levels, historic,height)\n\nI decided I wanted to plot this map in the shape of egg because 1) I was too lazy to change the CRS out of WGS84 and using a standard buffer on this will be elongated because Montpellier is at a 43 deg. latitude and 2) because La place de la Comédie in the historical center used to be called l’œuf, or “the egg”, because of the shape of the paving stones. I created a buffer and used this to mask the path and building data using st_intersection. I decided it needed a medieval-style font and settled on Morris Roman Black\n\nmtp.cent <- c(3.876616, 43.610607)\nmtp.buffer <- st_point(mtp.cent) %>%\n  st_sfc(crs = 4326) %>%\n  st_buffer(dist = 0.005)\n\n#crop with the nice circle\npath.crop <- st_intersection(mtp.path, mtp.buffer)\nbuild.crop <- st_intersection(mtp.building, mtp.buffer)\n\n\n#define caption for easier reading\ncaption.lab <- paste0(\"#30DayMapChallenge | \",\n                      \"<b>Source: </b>OpenStreetMap<br>\",\n                      \"<span style='font-family:fa-brands;'>&#xf113;</span> mvevans89\")\n\n\npng(\"centre-map.png\", res = 200, width = 600, height = 1100)\nggplot() +\n  geom_sf(data = mtp.buffer, fill = \"#E0C9A6\", color = \"gray30\", alpha = 0.6) +\n  geom_sf(data = build.crop, fill = \"#532915\", color = \"gray30\", size = 0.1, alpha = 0.4) +\n  # geom_sf(data = path.crop, color = \"#532915\", size = 0.6, alpha = 1) +\n  theme_void() +\n  coord_sf(clip = \"off\") +\n  labs(title= \"L'Ecusson\",\n       caption = caption.lab) +\n  geom_richtext(aes(x = 3.8766, y = 43.6047, label = \"<span style=;font-family:morris-roman;'>Le centre historique<br>de Montpellier</span>\"), fill = NA, label.color = NA,\n                color = \"black\", size = 5, hjust = 0.5, vjust = 0.5) +\n  theme(plot.title = element_text(family = \"morris-roman\", hjust = 0.5, size = 25),\n        plot.caption  = element_markdown(family = \"serif\", hjust = 0.5, size = 6))\ndev.off()"
  },
  {
    "objectID": "posts/2022-30daymapping/day09-space/index.html",
    "href": "posts/2022-30daymapping/day09-space/index.html",
    "title": "Mapping space",
    "section": "",
    "text": "Photo by Denis Degioanni on Unsplash\n\n\nFor the space theme, I wanted to make a map of outer space, specifically the Milky Way. For the data, I followed this super detailed blog post by Kim Fitter. It has the source for some maps of outer space from the d3-celestial package and how to trouble-shoot some issues with the data, specifically problems with dateline wrapping. If you’re interested in learning more about celestial mapping, check out that blog post! Below is the code to download and clean the map of the milky way, based on the post.\n\n#read in json with sf\nmw.url <- \"https://raw.githubusercontent.com/ofrohn/d3-celestial/master/data/mw.json\"\nmw.sf <- st_read(mw.url,stringsAsFactors = FALSE)\n\n#have to wrap the dateline to get rid of lines\nmilky.wrap <- mw.sf %>% \n       st_cast(\"MULTILINESTRING\") %>%  \n       st_cast(\"LINESTRING\") %>%\n       group_by(id) %>%  \n       st_wrap_dateline(options = c(\"WRAPDATELINE=YES\", \"DATELINEOFFSET=180\")) %>%\n  ungroup()\n\n#issue with some multi-polygons after the wrapping\nmilky.wrap[3:202,] <- milky.wrap[3:202,] %>% \n      st_cast(\"MULTIPOLYGON\") \n\n#transform to polar stereographic because it looks cool\nnew.crs <- 3995\nmilky.trans <- st_transform(milky.wrap, crs = new.crs)\n#these are fully enclosed polygons\nmilky.poly <- concaveman::concaveman(milky.trans[1:2,]) %>%\n  st_transform(crs = new.crs) %>%\n  st_zm()\n\nI then chose a spacey color palette, to set the theme of the map. Then I randomly assigned these colors and different alpha values to the differ polygon lines of the milky way sf object.\n\n#ordered dark to light\nspace.pal <- c(\"#9eadcf\", \"#7496c1\", \"#54526a\", \"#5754a8\", \"#34344d\")\n\nmilky.plot <- milky.trans %>%\n  mutate(randomcol = sample(letters[1:5], size = 202, replace = T)) %>%\n  mutate(randomalpha = runif(202))\n\nI also created a nice star background by generating 1000 random points within the bounding box and assigning them different values of gray so they would like look like they were different distances away, and therefore different levels of brightness.\n\nplot.bbox <- st_bbox(milky.plot)\nstars <- data.frame(x = runif(1000,plot.bbox[1],\n                              plot.bbox[3]),\n                    y = runif(1000, plot.bbox[2],\n                              plot.bbox[4])) %>%\n  #randomize color\n  mutate(randomcol = sample(paste0(\"gray\", seq(10,100, by = 10)),\n                            1000, replace = T))\n\nThen I plotted it, setting the background to black and using the fully enclosed polygons of the milky way to add depth by plotting it as a background with higher transparency. And of course, added a space-themed font for the title, this time Orbitron Black\n\n#easier to read caption\ncaption.lab <- paste0(\"#30DayMapChallenge <br>\",\n                      \"<b>Source: </b>d3 celestial<br>\",\n                      \"<span style='font-family:fa-brands;'>&#xf113;</span> mvevans89\")\n\n\nggplot()+\n  geom_point(data = stars, aes(x = x, y = y, color = randomcol), \n             size = 0.001) +\n  scale_color_manual(values = paste0(\"gray\", seq(10,100, by = 10)), \n                     guide = 'none') +\n  geom_sf(data = milky.poly, fill = space.pal[2], \n          alpha = 0.2, color = NA) +\n  ggnewscale::new_scale_color() +\n  geom_sf(data = milky.plot, aes(color = \"white\", fill = randomcol), \n          size = 0.2, alpha = 0.6) +\n  scale_fill_manual(values = space.pal, guide = 'none') +\n  scale_color_manual(values = space.pal, guide = 'none') +\n  labs(caption = caption.lab,\n       title = \"The Polar Milky Way\") +\n  theme(panel.background = element_rect(fill = \"black\", color = \"black\"),\n        panel.grid.major = element_line(color = \"gray20\", linetype = \"dotted\"),\n        axis.text = element_blank(),\n        axis.ticks = element_blank(),\n        plot.background = element_rect(fill = 'black', color = 'black'),\n        plot.caption  = element_markdown(family = \"sans\", hjust = 1, size = 9,\n                                         color = \"gray60\"),\n        plot.title = element_markdown(family = \"orbitron\", color = 'white', \n                                      hjust = 0.5, size = 18),\n        plot.margin = unit(rep(0.2,4), \"cm\"))"
  },
  {
    "objectID": "posts/2022-30daymapping/day15-food/index.html",
    "href": "posts/2022-30daymapping/day15-food/index.html",
    "title": "Hardee’s and Carl’s Jr. Great Divide",
    "section": "",
    "text": "Photo by Jonathan Borba on Unsplash\n\n\nThe topic for day 15 was food. If you’ve ever driven across the country, you may have had the Mandela-effect like feeling when you come across a Hardee’s or Carl’s Jr. They seem almost exactly the same: similar fonts, same smiling little star, and identical menus. But they are conceivably different restaurants? Long story short, they merged nearly twenty years ago, and kept the franchise names to not disrupt the markets and fan bases on their respective coasts. Essentially, it’s the second-most famous East vs. West coast beef, and I wanted to make a map of the dividing line based on distance to the nearest Hardee’s or Carl’s Jr.\nI used data from fastfoodmaps.com to get the location of all the restaurants, and then created rasters of Euclidean distance to each restaurant.\n\nfood.locs <- read.csv(\"fastfoodmaps_locations_2007.csv\", header = F) |>\n  #limit to just Hardee and Carls Jr\n  filter(V2 %in% c(\"c\", \"h\")) |>\n  #limit to just continentental US\n  filter(!(V5 %in% c(\"HI\", \"AK\"))) |>\n  select(V2, V8, V9) \n\nfood.sf <- st_as_sf(food.locs, coords = c(\"V9\", \"V8\"), crs = 4326)\n\n#create raster template\nusa.poly <- st_as_sf(maps::map(\"usa\", fill=TRUE, plot =FALSE)) |>\n  filter(ID == \"main\")\nusa.bbox <- st_bbox(usa.poly)\n\n#a finer resolution will make distance calculations take longer\nus.rast <- rast(crs = \"epsg:4326\", resolution = 0.33333, \n                xmin = usa.bbox$xmin, xmax = usa.bbox$xmax, \n                     ymin = usa.bbox$ymin, ymax = usa.bbox$ymax)\nvalues(us.rast) <- 1\n\ndist.h.base <- distance(x = us.rast, y = vect(filter(food.sf, V2 == \"h\")))\ndist.c.base <- distance(x = us.rast, y = vect(filter(food.sf, V2 == \"c\")))\ndist.h.base <- dist.h.base/1000\ndist.c.base <- dist.c.base/1000\n\np1 <- ggplot() +\n  geom_spatraster(data = mask(dist.h.base, vect(usa.poly))) +\n  ggtitle(\"Distance to Hardee's\") +\n  scale_fill_viridis_c(na.value = NA) +\n  theme_void()\np2 <- ggplot() +\n  geom_spatraster(data = mask(dist.c.base, vect(usa.poly))) +\n  ggtitle(\"Distance to Carl's Jr.\") +\n  scale_fill_viridis_c(na.value = NA) +\n  theme_void()\n\np1 + p2\n\n\n\n\nI then wanted mask out those pixels where the other restaurant was closer, essentially trying to create a raster for each side of the United States. I did this using some simple raster logic, where any pixels where the other restaurant was closer became NA.\n\n#create raster to identify which is closest\nh.closer <- dist.h.base<dist.c.base\n\ndist.c <- dist.c.base\ndist.h <- dist.h.base\ndist.c[h.closer] <- NA\ndist.h[!h.closer] <- NA\n#mask so it is just US outline\ndist.c.mask <- mask(dist.c, vect(usa.poly))\ndist.h.mask <- mask(dist.h, vect(usa.poly))\n\np1 <- ggplot() +\n  geom_spatraster(data = dist.h.mask) +\n  ggtitle(\"Distance to Hardee's\") +\n  scale_fill_viridis_c(na.value = NA) +\n  theme_void()\n\np2 <- ggplot() +\n  geom_spatraster(data = dist.c.mask) +\n  ggtitle(\"Distance to Carl's Jr.\") +\n  scale_fill_viridis_c(na.value = NA) +\n  theme_void()\n\np1 +p2\n\n\n\n\nThe last bit of spatial data creation I did was the creation of the dividing line between the two rasters. There are many ways to do this, but these are the steps I followed:\n\ncreated a polygon that represented the outline of all non-NA values in each raster using as.polygons\ncreated a buffer around that polygon via st_buffer\nidentified the overlap between the two buffered polygons using st_intersection\n\n\ncarl.poly.r <- dist.c.mask\ncarl.poly.r[!is.na(carl.poly.r)] <- 1\ncarl.poly <- as.polygons(carl.poly.r, dissolve = T) %>%\n  st_as_sf() %>%\n  st_buffer(dist = 20000)\n\nhard.poly.r <- dist.h.mask\nhard.poly.r[!is.na(hard.poly.r)] <- 1\nhard.poly <- as.polygons(hard.poly.r, dissolve = T) %>%\n  st_as_sf() %>%\n  st_buffer(dist = 20000)\n\n#find commonality to get dividing line\nboundary <- st_intersection(hard.poly, carl.poly)\n\nFinally, it was just plotting it all:\n\n#define some things outside of the plot call for easier readability\nbg.col <- \"gray10\"\nfact.caption <- \"Hardee's and Carl's Jr. merged in 1997. The menu is now the same, but which one you eat at depends on where you live.\"\ncaption.lab <- paste0(\"#30DayMapChallenge \",\n                      \"<b> Source: </b>fastfoodmaps.com &nbsp; &nbsp;  | &nbsp;  &nbsp; \",\n                      \"<span style='font-family:fa-brands;'>&#xf113;</span> mvevans89\")\n\n\nggplot() +\n    geom_spatraster(data = dist.h.mask) +\n  scale_fill_distiller(palette = \"Blues\", na.value = NA, name = \"Distance to Hardee's\") +\n    ggnewscale::new_scale_fill() +\n  geom_spatraster(data = dist.c.mask) +\n  scale_fill_distiller(palette = \"Reds\", na.value = NA, name = \"Distance to Carl's Jr.\")+\n  geom_sf(data = boundary, color = \"black\", fill = \"white\") +\n  labs(title = \"A Nation Divided\",\n       caption = caption.lab) +\n  annotate(geom = \"text\", x = -120, y = 27, label = stringr::str_wrap(fact.caption, 35),\n           col = \"white\", size = 4) +\n  geom_richtext(aes(label = \"<img src='carls.png' width='80'/>\", x = -110, y = 35), \n                fill = NA, color = NA) +\n    geom_richtext(aes(label = \"<img src='hardees.png' width='80'/>\", x = -85, y = 38), \n                fill = NA, color = NA) +\n  theme_void() +\n  theme(legend.position = \"bottom\",\n        panel.background = element_rect(fill = bg.col, color = NA),\n        plot.background = element_rect(fill = bg.col, color = NA),\n        legend.background = element_rect(fill = bg.col, color = NA),\n        legend.text = element_text(color = \"white\"),\n        legend.title = element_text(color = \"white\"),\n        plot.caption  = element_markdown(family = \"sans\", hjust = 0.5, size = 9,\n                                         color = \"gray80\"),\n        plot.title = element_text(color = \"white\", size = 24, hjust = 0.5),\n        legend.direction = \"horizontal\")"
  },
  {
    "objectID": "posts/2022-30daymapping/day16-minimal/index.html",
    "href": "posts/2022-30daymapping/day16-minimal/index.html",
    "title": "Contour maps in ggplot2",
    "section": "",
    "text": "Source: wikiwand.com\nToday’s map theme is ‘minimal’, and I’ve chosen to create a contour map of Antananarivo, Madagascar. Antananarivo is the capital of Madagascar. Although it’s name translate as “a thousand towns”, my experience walking around it would better describe it as “a thousand hills”. Like much of Madagascar, it is in a mountainous highland region, and is what one might call topographically complex. Luckily, there are many, sometimes steep, staircases to help everyone get around, including a famous set of staircases near Anakaley market (below)."
  },
  {
    "objectID": "posts/2022-30daymapping/day16-minimal/index.html#general-workflow-for-creating-contour-maps-in-r",
    "href": "posts/2022-30daymapping/day16-minimal/index.html#general-workflow-for-creating-contour-maps-in-r",
    "title": "Contour maps in ggplot2",
    "section": "General workflow for creating contour maps in R",
    "text": "General workflow for creating contour maps in R\nContour maps are a type of topographic map that show changes in elevation using contour lines drawn at different values of elevation. Before the use of computer-generated raster images, they were the most common way of showing elevation in a method that could be hand-drawn. These maps are still used today, and what we often imagine when we think of topographic paper maps.\nGenerating these maps in R can be very straight-forward because ggplot already has a geom that is expressely made for this purpose, called geom_contour. All you need is a raster image to plot. Again, there is a package for this! Elevatr is R package that facilitates the downloading of elevation raster images."
  },
  {
    "objectID": "posts/2022-30daymapping/day16-minimal/index.html#an-example-of-plotting-contour-lines-antananarivo",
    "href": "posts/2022-30daymapping/day16-minimal/index.html#an-example-of-plotting-contour-lines-antananarivo",
    "title": "Contour maps in ggplot2",
    "section": "An example of plotting contour lines: Antananarivo",
    "text": "An example of plotting contour lines: Antananarivo\n\nDownload the elevation raster\nThere are several ways to download raster images via the elevatr package. For now, we will focus on downloading the Mapzen composite that combines multiple datasets to estimate elevation because it doesn’t require setting up an account to download. For Antananarivo, this essentially ends up being SRTM data, because it is the only dataset available for the region. You can choose the image to download by providing coordinates or an sp object. Here, we will provide the longitude and latitude of Antananarivo to the get_elev_raster function.\nThe Mapzen data also requires a zoom level, where increasingly high numbers corresponding to increasingly high resolution (smaller pixels). The actually values of the resolution are noted on their github. We will use a zoom of 12, which corresponds to a resolution fo 38m at the equator.\nWe can also provide a projection or coordinate reference system (CRS), so that the image is immediately able to plotted in geographic space. For convenience, we will just use the WGS84 projection, described via the string \"+proj=longlat +datum=WGS84\".\n\ntana.coords <- data.frame(x = 47.5, y = -18.88)\n\ntana.dem <- get_elev_raster(locations = tana.coords,\n                            z = 12,\n                            prj = \"+proj=longlat +datum=WGS84\",\n                            source = \"aws\")\n\n\n\n\n\n\n\nWe can then investigate the raster by simply plotting it to have an idea of what it looks like:\n\nplot(tana.dem)\n\n\n\n\nIt is like any other raster in R, so we can also just call the object to see some of its characteristics:\n\ntana.dem\n\nclass      : RasterLayer \ndimensions : 498, 526, 261948  (nrow, ncol, ncell)\nresolution : 0.0001671195, 0.0001671195  (x, y)\nextent     : 47.46094, 47.54884, -18.89594, -18.81272  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 \nsource     : file52631dc1f2f.tif \nnames      : file52631dc1f2f \nvalues     : -32768, 32767  (min, max)\n\n\n\n\nPlotting the contour lines\nTo plot the raster in ggplot and plot the contour lines, we first have to fortify it into a dataframe. If you don’t first fortify it, you will get the following error message:\n\nggplot(data = tana.dem) +\n  geom_contour()\n\nError in `fortify()`:\n! `data` must be a data frame, or other object coercible by `fortify()`, not an S4 object with class RasterLayer.\n\n\nFortifying a raster is similar to turning a wide dataset into a long dataset. We create a row for each cell that has three columns: the x coordinate, the y coordinates, and the pixel value (z). We can use the function as.data.frame with xy=T to accomplish this:\n\ntana.df <- as.data.frame(tana.dem, xy = T)\n\nhead(tana.df)\n\n         x        y file52631dc1f2f\n1 47.46102 -18.8128            1247\n2 47.46119 -18.8128            1247\n3 47.46136 -18.8128            1247\n4 47.46152 -18.8128            1247\n5 47.46169 -18.8128            1247\n6 47.46186 -18.8128            1248\n\n\nWe can see it has created columns x and y and a column corresponding to the names attribute of the raster which holds the value of each pixel. The name for this particularly file is pretty nonsensical, so I would recommend renaming it to something that is shorter and means something, like ‘elev’ for elevation. Now we can use this like any other dataframe that we would give to ggplot. With geom_tile we can use the elevation to describe the fill creating an image where the color of a pixel corresponds to the elevation. Then, on top of that, we can use geom_contour to make some contour lines, using elevaation for the z argument.\n\ntana.df <- rename(tana.df, elev = file52631dc1f2f)\n\nggplot(tana.df, aes(x = x, y = y)) + \n  geom_tile(aes( fill = elev)) +\n  geom_contour(aes(z = elev))\n\n\n\n\nThe defaults for this are a bit difficult to see, so we can change some of the aesthetics and color to help with visualization. We can also change the breaks for the contour lines, so they aren’t so close together. This is done via the bins argument to geom_contour.\n\nggplot(tana.df, aes(x = x, y = y)) +\n  geom_tile(aes(fill = elev)) +\n  geom_contour(aes(z = elev), bins = 8, color = \"white\")\n\n\n\n\nNow we can see how it highlights some of the really steep areas of the city.\n\n\nCreating the minimal map\nFor the ‘minimal’ map theme, I just want to plot the contour lines with a blank background. This can be achieved by changing some of ggplots themes, and not using the geom_tile we used in the earlier plots. For the colors, I will use some of the palettes from CARTO. I’m particularly fond of the Mint palette, but for this I chose two colors from the Fall palette.\nThen I added a title the map. Because I want it kind of in the middle, between some of the hills, I chose to use annotate to add the text, rather than added a formal title, which would by default be at the top of the plot.\n\nggplot(tana.df, aes(x = x, y = y)) +\n  geom_contour(aes(z = elev), bins = 8, color = \"#3d5941\", size = 0.5) +\n  #drop all the plot oulines, axes, and grid lines\n  theme_void() +\n  #remove margins\n  scale_x_continuous(expand=c(0,0))+\n  scale_y_continuous(expand=c(0,0))+\n  annotate(geom = \"text\", x = 47.518, y = -18.825, \n           label = \"Antananarivo\", color = \"#A16928\",\n           family = \"serif\", size = 7) +\n  annotate(geom = \"text\", x = 47.518, y = -18.828,\n           label= \"18.8792° S, 47.5079° E\",color = \"#A16928\",\n           family = \"serif\", size = 5\n           ) +\n  labs(caption =  paste0(\"#30DayMapChallenge <> \",\n                      \"<b> Source: </b> SRTM <> \",\n                      \"<span style='font-family:fa-brands;'>&#xf113;</span> mvevans89\")) +\n  theme(plot.background = element_rect(fill = \"#f6edbd\", color = NA),\n        plot.caption = element_markdown(color = \"#A16928\", family = \"serif\", hjust = 0.5))"
  },
  {
    "objectID": "posts/2022-30daymapping/day16-minimal/index.html#extracing-contour-lines-as-spatial-objects",
    "href": "posts/2022-30daymapping/day16-minimal/index.html#extracing-contour-lines-as-spatial-objects",
    "title": "Contour maps in ggplot2",
    "section": "Extracing contour lines as spatial objects",
    "text": "Extracing contour lines as spatial objects\nThe nice thing about this workflow is that we don’t have to create any of the contour lines ourselves, this is all done under the hood by ggplot. But what if you want to extract the lines to use as polygons in another map or to perform some spatial analyses on? We can do this using the raterToContour function from the raster. Note it can also be done with the stars package, but this requires GDAL version 2.4 or greater. This works example the same as our ggplot call, where we provide the raster and the number of bins or levels (remembering to do one fewer). The way the breaks are drawn may be slightly different, so the best way to ensure they are the same is to supply the actual breaks yourself, rather than the number of bins or levels. The function returns spatial lines that can then be plotted or analysed.\n\ntana.contour <- rasterToContour(tana.dem, nlevels = 7)\n\nplot(tana.contour)\n\n\n\ntana.contour\n\nclass       : SpatialLinesDataFrame \nfeatures    : 5 \nextent      : 47.46107, 47.54871, -18.89581, -18.81285  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 \nvariables   : 1\nnames       : level \nmin values  :  1240 \nmax values  :  1320 \n\n\nHappy mapping!"
  },
  {
    "objectID": "posts/2022-30daymapping/day16-minimal/index.html#extracting-contour-lines-as-spatial-objects",
    "href": "posts/2022-30daymapping/day16-minimal/index.html#extracting-contour-lines-as-spatial-objects",
    "title": "Contour maps in ggplot2",
    "section": "Extracting contour lines as spatial objects",
    "text": "Extracting contour lines as spatial objects\nThe nice thing about this workflow is that we don’t have to create any of the contour lines ourselves, this is all done under the hood by ggplot. But what if you want to extract the lines to use as polygons in another map or to perform some spatial analyses on? We can do this using the raterToContour function from the raster. Note it can also be done with the stars package, but this requires GDAL version 2.4 or greater. This works example the same as our ggplot call, where we provide the raster and the number of bins or levels (remembering to do one fewer). The way the breaks are drawn may be slightly different, so the best way to ensure they are the same is to supply the actual breaks yourself, rather than the number of bins or levels. The function returns spatial lines that can then be plotted or analysed.\n\ntana.contour <- rasterToContour(tana.dem, nlevels = 7)\n\nplot(tana.contour)\n\n\n\ntana.contour\n\nclass       : SpatialLinesDataFrame \nfeatures    : 5 \nextent      : 47.46107, 47.54871, -18.89581, -18.81285  (xmin, xmax, ymin, ymax)\ncrs         : +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0 \nvariables   : 1\nnames       : level \nmin values  :  1240 \nmax values  :  1320 \n\n\nHappy mapping!"
  },
  {
    "objectID": "series-30daymap2022.html",
    "href": "series-30daymap2022.html",
    "title": "Series: #30DayMappingChallenge 2022",
    "section": "",
    "text": "Map projections in ggplot\n\n\n\n\n\nDay 19 of the #30DayMapChallenge - Globe\n\n\n\n\n\n\nJan 17, 2023\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nContour maps in ggplot2\n\n\n\n\n\nDay 16 of the #30DayMapChallenge - Minimal\n\n\n\n\n\n\nNov 16, 2022\n\n\n7 min\n\n\n\n\n\n\n  \n\n\n\n\nHardee’s and Carl’s Jr. Great Divide\n\n\n\n\n\nDay 15 of the #30DayMapChallenge - Food and Drink\n\n\n\n\n\n\nNov 15, 2022\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nMapping space\n\n\n\n\n\nDay 9 of the #30DayMapChallenge - Space\n\n\n\n\n\n\nNov 9, 2022\n\n\n6 min\n\n\n\n\n\n\n  \n\n\n\n\nCreating a streetmap of Montpellier\n\n\n\n\n\nDay 8 of the #30DayMapChallenge - OpenStreetMap\n\n\n\n\n\n\nNov 8, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nAdding images to plots with ggtext\n\n\n\n\n\nDay 7 of the #30DayMapChallenge - Raster\n\n\n\n\n\n\nNov 7, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nUkraine: The Center of Europe\n\n\n\n\n\nDay 5 of the #30DayMapChallenge - Ukraine\n\n\n\n\n\n\nNov 5, 2022\n\n\n4 min\n\n\n\n\n\n\n  \n\n\n\n\nExploring Rainforests with Rayshader\n\n\n\n\n\nDay 4 of the #30DayMapChallenge - Something Green\n\n\n\n\n\n\nNov 4, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nCongressional Districts over time\n\n\n\n\n\nDay 3 of the #30DayMapChallenge - Polygons\n\n\n\n\n\n\nNov 3, 2022\n\n\n3 min\n\n\n\n\n\n\n  \n\n\n\n\nAM-TRON\n\n\n\n\n\nDay 2 of the #30DayMapChallenge - Lines\n\n\n\n\n\n\nNov 2, 2022\n\n\n2 min\n\n\n\n\n\n\n  \n\n\n\n\nC’est où, Paris?\n\n\n\n\n\nDay 1 of the #30DayMapChallenge - Points\n\n\n\n\n\n\nNov 1, 2022\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-30daymapping/day19-globe/index.html",
    "href": "posts/2022-30daymapping/day19-globe/index.html",
    "title": "Map projections in ggplot",
    "section": "",
    "text": "You wanna change… the map? Source: The West Wing\nAs anyone familiar with Aaron Sorkin’s West Wing knows, map projections are biased. In one episode of the West Wing, the fictitious Organization of Cartographers for Social Equality make the point that the commonly used Mercator map projection distorts the sizes of continents in favor of Europe and North America, essentially making the southern hemisphere continents seem much smaller than they actually are and presenting a eurocentric view of the world. They propose an alternate map projection: the Gall-Peters projection, which has an astonishingly detailed wikipedia page."
  },
  {
    "objectID": "posts/2022-30daymapping/day19-globe/index.html#providing-a-coordinate-reference-system-crs",
    "href": "posts/2022-30daymapping/day19-globe/index.html#providing-a-coordinate-reference-system-crs",
    "title": "Map projections in ggplot",
    "section": "Providing a coordinate reference system (CRS)",
    "text": "Providing a coordinate reference system (CRS)\nEither method takes one of the following arguments for the coordinate reference system (CRS):\n\nobject of class crs created via the st_crs function. This might be used if you wanted to match the CRS of another dataset, just calling st_crs(second_data) as the argument.\ninput string that st_crs can read. The format of this can vary widely. I most often reference a CRS using its EPSG number, as above. There is a database of all EPSG codes at epsg.io and you can even look up the most commonly used CRS for each country. This can then be provided in numeric (e.g. 3857) or a string format (e.g. “EPSG:3857”). Some common EPSG codes are:\n\n\n\n\nEPSG\nCRS Name\n\n\n\n\n4326\nWGS 84\n\n\n3857\nWeb Mercator\n\n\n4269\nNAD83\n\n\n6842\nMODIS Sinusoidal\n\n\n\nSometimes there may not be a code for a certain CRS you want to use, or the code may not exist in the GDAL EPSG support files. Then you can provide a longer proj-string with specific information about things like the datum, ellipsoid, projection, true scales, etc. These are referred to as PROJ.4 strings in the EPSG database. For example, the full proj-string for WGS 84 (EPSG 4326) is +proj=longlat +datum=WGS84 +no_defs +type=crs."
  },
  {
    "objectID": "pubs.html#section",
    "href": "pubs.html#section",
    "title": "Academic Publications",
    "section": "2023",
    "text": "2023\n Morbidity and mortality burden of COVID-19 in rural Madagascar: results from a longitudinal cohort and nested seroprevalence study  PDF  Garchitorena A, LT Rasoloharimanana, R JL Rakotonanahary, MV Evans, AC Miller, KE Finnegan, LF Cordier, G Cowley, B Razafinjato, M Randriamanambintosa, S Andrianambinina, SK Popper, R Hotahiene, MH Bonds, M Schoenhals. 2023. International Journal of Epidemiology. doi:10.1093/ije/dyad135. \n Socio-demographic variables can guide prioritized testing strategies for epidemic control in resource-limited contexts  PDF  Evans MV, T Ramiadantsoa, K Kauffman, J Moody, CL Nunn, JY Rabezara, P Raharimalala, TK Randriamoria, V Soarimalala, G Titcomb, A Garchitorena, B Roche. 2023. The Journal of Infectious Diseases. doi: 10.1093/infdis/jiad076.\n Climatic, land-use and socio-economic factors can predict malaria dynamics at fine spatial scales relevant to local health actors: Evidence from rural Madagascar  Pourtois JD, K Tallam, I Jones, E Hyde, AJ Chamberlin, MV Evans, FA Ihantamalala, LF Cordier, BR Razafinjato, R JL Rakotonanahary, TA Aina, P Soloniaina, SH Raholiarimanana, C Razafinjato, MH Bonds, GA de Leo, SH Sokolow, A Garchitorena. 2023. PLoS Global Public Health. doi:10.1371/journal.pgph.0001607. \n The mismatch of narratives and local ecologies in the everyday governance of water access and mosquito control in an urbanizing community  PDF Evans MV, S Bhatnagar, JM Drake, CC Murdock, JL Rice, S Mukherjee. 2023. Health and Place. doi:10.1016/j.healthplace.2023.102989. \n\n2022\n Geographic barriers to care persist at the community healthcare level: Evidence from rural Madagascar  Evans MV*, T Andréambeloson*, M Randriamihaja, F Ihantamalala, L Cordier, G Cowley, K Finnegan, F Hanitriniaina, AC Miller, LM Ralantomalala, A Randriamahasoa, B Razafinjato, E Razanahanitriniaina, R JL Rakotonanahary, IJ Andriamiandra, MH Bonds, A Garchitorena. 2022. PLoS Global Public Health. doi:10.1371/journal.pgph.0001028. *Contributed equally.\n A data-driven horizon scan of bacterial pathogens at the wildlife-livestock interface  Evans MV, JM Drake. 2022. EcoHealth. doi:10.1007/s10393-022-01599-3. \n Prioritizing COVID-19 vaccination efforts and dose allocation within Madagascar  Rasambainarivo F, T Ramiadantsoa, A Raherinandrasana, S Randrianarisoa, BL Rice, MV Evans, B Roche, FM Randriatsarafara, A Wesolowski, JC Metcalf. Prioritizing COVID-19 vaccination efforts and dose allocation within Madagascar. BMC Public Health. doi:10.1186/s12889-022-13150-8.\n Socio-ecological dynamics in urban systems: An integrative approach to mosquito-borne disease in Bengaluru, India  Evans MV, S Bhatnagar, JM Drake, CC Murdock, S Mukherjee. 2022. People & Nature. doi:10.1002/pan3.10311.\n Both consumptive and non-consumptive effects of predators impact mosquito populations and have implications for disease transmission  Russell MC*, CM Herzog*, Z Gajewski, C Ramsay, F El Moustaid, MV Evans, T Desai, NL Gottdenker, SL Hermann, AG Power, AG McCall. 2022. eLife. doi:10.7554/eLife.71503. *Contributed equally.\n\n\n2021\n Assessing temperature-dependent competition between two invasive mosquito species  PDF Evans MV, JM Drake, L Jones, CC Murdock. 2021. Ecological Applications. doi:10.1002/eap.2334.. \n Integrating health systems and science to respond to COVID-19 in a model district of rural Madagascar  Rakotonanahary RJL, H Andriambolamanana, B Razafinjato, EM Raza-Fanomezanjanahary, V Ramanandraitsiory, et al. Matthew H. Bonds. 2021. Frontiers in Public Health. doi: 10.3389/fpubh.2021.654299. \n Socio-demographic, not environmental, risk factors explain fine-scale spatial patterns of diarrhoeal disease in Ifanadiana, rural Madagascar  Evans MV, MH Bonds, LF Cordier, JM Drake, F Ihantamalala, J Haruna, AC Miller, CC Murdock, M Randriamanambtsoa, EM Raza-Fanomezanjanahary, BR Razafinjato, AC Garchitorena. 2021. Proceedings of the Royal Society B. doi:10.1098/rspb.2020.2501.\n\n\n2020\n Land cover affects microclimate and temperature suitability for arbovirus transmission in an urban landscape  Wimberly M, JK Davis, MV Evans, A Hess, PM Newberry, N Solano-Asamoah, and CC Murdock. 2020. PLoS NTD. doi:10.1080/16549716.2020.1816044. \n Reconciling model predictions with low reported cases of COVID-19 in Sub-saharan Africa: Insights from Madagascar  Evans MV, A Garchitorena, RJL Rakotonanahary, JM Drake, B Andriamihaja, E Rajaonarifara, CN Ngonghala, B Roche, MH Bonds, J Rakotonirina. 2020. Global Health Action. doi:10.1080/16549716.2020.1816044. \n Mosquito-virus interactions  No PDF Available Reitmayer CM, MV Evans, KL Miazgowicz, PM Newberry, N Solano-Asamoah, B Tesla, and CC Murdock. 2020. In: J. M. Drake, M. Strand, M. Bonsall (Eds.), Population Biology of Vector-Borne Diseases. Oxford University Press. Book Chapter.\n Carry-over effects of the larval environment in mosquito-borne disease systems PDF Evans MV, PM Newberry, CC Murdock. 2020. In: J. M. Drake, M. Strand, M. Bonsall (Eds.), Population Biology of Vector-Borne Diseases. Oxford University Press. Book Chapter.\n\n\n2019\n Microclimate and larval habitat density predict adult Aedes albopictus abundance in urban areas  Evans MV, CW Hintz, L Jones, J Shiau, N Solano, JM Drake, CC Murdock. 2019. The American Journal of Tropical Medicine and Hygiene. doi:10.4269/ajtmh.19-0220. \n\n\n2018\n Spatio-temporal spillover risk of yellow fever in Brazil  Kaul RB*, MV Evans*, CC Murdock, JM Drake. 2018. Parasites & Vectors. doi:10.1186/s13071-018-3063-6.. *Contributed equally. \n Carry-over effects of urban larval environments on the transmission potential of dengue-2 virus  Evans MV, JC Shiau, N Solano, MA Brindley, JM Drake, CC Murdock. 2018. Parasites & Vectors. doi:10.1186/s13071-018-3013-3. \n Anticipating emerging mosquito-borne flaviviruses in the USA: What comes after Zika? PDF Evans MV, CC Murdock, JM Drake. 2018. Trends in Parasitology.  doi:10.1016/j.pt.2018.02.010. \n\n\n2017\n Fine-scale variation in microclimate across an urban landscape shapes variation in mosquito population dynamics and the potential of Aedes albopictus to transmit arboviral disease  Murdock CC, MV Evans, TD McClanahan, KL Miazgowicz, B Tesla. 2017. PLoS NTD.  doi:10.1371/journal.pntd.0005640. \n Detecting the impact of temperature on transmission of Zika, dengue and chikungunya using mechanistic models  Mordecai E, J Cohen, MV Evans, P Gudapati, LR Johnson, CA Lipp, K Miazgowicz, CC Murdock, JR Rohr, SJ Ryan, V Savage, M Shocket, AS Ibarra, MB Thomas, DP Weikel. 2017. PLoS NTD. doi:10.1371/journal.pntd.0005568. \n Data-driven identification of potential Zika virus vectors  Evans MV, TA Dallas, BA Han, CC Murdock, JM Drake. 2017. eLife. doi:10.7554/eLife.22053."
  }
]