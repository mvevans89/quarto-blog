{
  "hash": "df21bbe0c8edb3e6a94df5b13946ef3b",
  "result": {
    "markdown": "---\ntitle: \"Dummy Variables for Dummies\"\ndescription: \"How to create dummy variables within a dataframe\"\nauthor: \"Michelle Evans\"\ndate: \"2021-02-27\"\ndraft: false\nimage: dummy-book.jpg\n---\n\n\n\nAs you may have guessed from the cliched title that I couldn't resist, this post is about dummy variables. Specifically, comparing a couple of different ways to create them in R, and seeing which is fastest.\n\n\n\n\n\n# What are dummy variables?\n\nDummy variables, sometimes called indicator variables, are a way to encode categorical variables as numerical (often binomial), so that we can model them in a regression. Luckily, R will often deal with creating these dummy variables under the hood when we use something like `lm` to model a regression, but this is not true for all methods.\n\nAs an example, let's say we are interested in the effect of three different types of crops (corn, wheat, rice) on soil nitrogen content (% N). The dataset may look something like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn.sample <- 100000 #how many rows of data will you have?\ncrop.types <- c(\"corn\", \"rice\", \"wheat\") #what are the levels of your categorical variable?\nnitrogen.data <- data.frame(\n  crop_type = crop.types[sample(1:3, n.sample, replace = T)],\n  perc_N = runif(n.sample, min = 0, max =1 )\n)\nhead(nitrogen.data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  crop_type     perc_N\n1     wheat 0.10717736\n2      rice 0.73282274\n3      rice 0.58827136\n4     wheat 0.08912314\n5      corn 0.62209752\n6      rice 0.29661595\n```\n:::\n:::\n\n\nIn R, we may model this in a regression framework with the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(perc_N ~ 0 + crop_type,\n   data = nitrogen.data)\n```\n:::\n\n\nThis corresponds to a regression equation that may look something like this:\n\n$y_i = \\beta_{1}corn_i + \\beta_2wheat_i + \\beta_3rice_i +\\epsilon$\n\nwhere:\n\n- $y_i$ is the % N of the field, *i* </br>\n\n- $\\beta_1$, $\\beta_2$, $\\beta_3$ are the coefficients for the effects of corn, wheat, and rice, respectively\n\n- $corn_i$, $wheat_i$, $rice_i$ are all binomial variables (0/1) for the type of crop grown in field *i*.\n\nThe model formula in the call to `lm` looks much simpler than this. That is because, under the hood, R is creating dummy variables from our `crop_type` variable, creating a binomial variable for each level of our categorical variable, without us needing to specify it.\n\n\n![](dummy_crop-type.png)\n\nWhen recording our data, it is much easier to do it like the data frame on the left, which is more human readable. Then we can turn it into the data frame on the right programmatically using R. This blog post will go over several ways to do this:\n\n1. the `model.matrix` function in base R\n2. via the `pivot_wider` function of the `tidyr` package (tidyverse framework)\n3. ~the`dummies` package~ [removed from CRAN as of April 2022]\n4. the `fastDummies`package\n\nAnd then compare the speeds on a larger dataset.\n\n# Using base R\n\nIf you don't want to install any additional packages, you can do this in base R via the `model.matrix` function. It takes as an argument a model formula, but note that you need to add the `~ 0 +` to the formula so that it doesn't include an intercept when creating the dummy data frame. This will create a matrix without the response variable, so you will may want to join it back to the original dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbase.mat <- model.matrix(object = ~0 + crop_type, data = nitrogen.data)\n#join with original data\nbase.df <- cbind(nitrogen.data, base.mat) \n\nhead(base.df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  crop_type     perc_N crop_typecorn crop_typerice crop_typewheat\n1     wheat 0.10717736             0             0              1\n2      rice 0.73282274             0             1              0\n3      rice 0.58827136             0             1              0\n4     wheat 0.08912314             0             0              1\n5      corn 0.62209752             1             0              0\n6      rice 0.29661595             0             1              0\n```\n:::\n:::\n\n\nThis is essentially what is happening behind the scenes when you do a linear regression without creating a dummy variable first. In fact the formula used in this call is the same as the one above.\n\n# Using the tidyverse\n\nYou can also do this using the `pivot_wider` function in `tidyr`. Using this method, you create a dummy_value column with all 1's and then spread that value across each of the categories from the `names_from` column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'dplyr' was built under R version 4.1.2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary(tidyr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'tidyr' was built under R version 4.1.2\n```\n:::\n\n```{.r .cell-code}\npivot_df <- nitrogen.data %>%\n  #create dummy value column\n  mutate(dummy_val = 1) %>%\n  #spread across crop type\n  pivot_wider(names_from = crop_type, values_from = dummy_val, values_fill = 0)\n\nhead(pivot_df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 Ã— 4\n  perc_N wheat  rice  corn\n   <dbl> <dbl> <dbl> <dbl>\n1 0.107      1     0     0\n2 0.733      0     1     0\n3 0.588      0     1     0\n4 0.0891     1     0     0\n5 0.622      0     0     1\n6 0.297      0     1     0\n```\n:::\n:::\n\n\nNote that we also supply the value to fill in a cell when a row doesn't correspond to that category, via the `values_fill` argument. \n\n# Using the `fastDummies` package\n\nThe play dataset we are using for this is only 100,000 rows and we are only concerned with one categorical variable with three levels, but real data may have many more rows and more complicated categorical variables, which can take much longer to turn into dummy variables. Hence the [`fastDummies` package](https://jacobkap.github.io/fastDummies/).\n\nUsing this package is super simple, and relies on one function to create dummy variables across columns. The function will by default turn all character or categorical variables into dummy variables, but you can also specify the columns you would like to \"dummify\" with the `select_columns` argument. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fastDummies)\nfastdummy.df <- dummy_cols(nitrogen.data, select_columns = \"crop_type\")\n\nhead(fastdummy.df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  crop_type     perc_N crop_type_corn crop_type_rice crop_type_wheat\n1     wheat 0.10717736              0              0               1\n2      rice 0.73282274              0              1               0\n3      rice 0.58827136              0              1               0\n4     wheat 0.08912314              0              0               1\n5      corn 0.62209752              1              0               0\n6      rice 0.29661595              0              1               0\n```\n:::\n:::\n\n\n\n# Comparing the speed\n\nThere isn't much difference between these methods code-wise, except the tidyverse method which is a couple of lines longer. Using `pivot_wider` also probably scales-up the worst, as you will need to repeat the technique for every categorical variable column you have, while the other methods allow you specify additional columns in the function itself. So we'll use `microbenchmark` to see which method is fastest when applied to a larger dataset of 500k rows and a categorical variable with 26 levels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn.sample = 500000\nfast.data <- data.frame(response = rnorm(n.sample, 0,1),\n                        category = sample(LETTERS, n.sample, replace = T))\n\ntime.test <- microbenchmark(\n  base = cbind(fast.data, model.matrix(object = ~0 + category, data = fast.data)),\n  tidyr_way = fast.data %>% mutate(dummy_val = 1) %>% pivot_wider(names_from = category, values_from = dummy_val, values_fill = 0),\n  fast_dummies = dummy_cols(fast.data, select_columns = \"category\"),\n  times = 20\n)\n```\n:::\n\n\n\n\nSurprisingly, the method using `tidyr` was actually the fastest way to create dummy variables, even though tidyverse is often critiqued for being relatively slow. It also required the most code and wasn't as clean of a workflow as the others, so there is a trade-off there. It was only about 3x slower than `fastDummies`, which is another relatively fast option when creating dummy variables for a larger dataset.\n\nI personally like the ability to pipe into the tidyr method within a tidyverse workflow and will probably continue to use that since it is the fastest, even if it is a couple of more lines of code.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}